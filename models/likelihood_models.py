import numpy as np

def BaseLine_LLH(param, Data): 
    n = int(np.max(Data[:, 0]))
    llh = 0
    for participant in range(1, n + 1): 
        trial_num = np.max(Data[Data[:, 0] == participant, 1])
        llh += (trial_num - 2) * np.log(0.5)
    return -llh

def RL_LLH(param, Data): 
    """
    This function is the loglikelihood function for a simple RL model with both state and action 
    build into the decision process. Specifically, the Q function takes as input both the block 
    currently presented to the participant and the action available to them. 
    """
    assert param.shape[0] == 2, 'Invalid amount of parameters'
    beta = param[0]
    alpha = param[1] 
    n = int(np.max(Data[:, 0])) # number of participants 
    llh = 0 # log-likelihood 
    k = 0 # iteration tracker
    blocks = {"BP": 0, "BS": 1, "WP": 2, "WS": 3}
    
    for participant in range(1, n + 1): 
        # set up Q-function
        Q = np.full((2, 4), np.nan)
        Q[0, :] = 0.0
        Q[1, :] = -0.5
        for trial in Data[Data[:, 0] == participant, 1]: 
            # set trial outcomes 
            action = Data[k, 2]
            reward = Data[k, 3]
            block = Data[k, 4]
            block_idx = blocks[block]
            # ignore the first two trials for loglikelihood computation
            if trial > 2: 
                # compute softmax probabilities
                softmax = np.full(2, np.nan)
                softmax[0] = 1 / np.sum(np.exp(beta * (Q[:, block_idx] - Q[0, block_idx])))
                softmax[1] = 1 - softmax[0]
                # update the loglikelihood 
                if softmax[int(action)] == 0: 
                    softmax[int(action)] = 0.000001
                llh += np.log(softmax[int(action)])
            # update Q-value if the block was approached
            if action == 1: 
                Q[action, block_idx] += alpha * (reward - Q[action, block_idx])
            # update iteration counter
            k += 1
            
    return -llh

def RL2a_LLH(param, Data): 
    """
    This function is the loglikelihood function for a simple RL model with both state and action 
    build into the decision process. Specifically, the Q function takes as input both the block 
    currently presented to the participant and the action available to them. 
    """
    assert param.shape[0] == 3, 'Invalid amount of parameters'
    beta = param[0]
    alpha_pos = param[1] 
    alpha_neg = param[2]
    n = int(np.max(Data[:, 0])) # number of participants 
    llh = 0 # log-likelihood 
    k = 0 # iteration tracker
    blocks = {"BP": 0, "BS": 1, "WP": 2, "WS": 3}
    
    for participant in range(1, n + 1): 
        # set up Q-function
        Q = np.full((2, 4), np.nan)
        Q[0, :] = 0.0
        Q[1, :] = -0.5
        for trial in Data[Data[:, 0] == participant, 1]: 
            # set trial outcomes 
            action = Data[k, 2]
            reward = Data[k, 3]
            block = Data[k, 4]
            block_idx = blocks[block]
            # ignore the first two trials for loglikelihood computation
            if trial > 2: 
                # compute softmax probabilities
                softmax = np.full(2, np.nan)
                softmax[0] = 1 / np.sum(np.exp(beta * (Q[:, block_idx] - Q[0, block_idx])))
                softmax[1] = 1 - softmax[0]
                # update the loglikelihood 
                if softmax[int(action)] == 0: 
                    softmax[int(action)] = 0.000001
                llh += np.log(softmax[int(action)])
            # update Q-value if the block was approached
            if action == 1: 
                if reward > 0: 
                    Q[action, block_idx] += alpha_pos * (reward - Q[action, block_idx])
                else: 
                    Q[action, block_idx] += alpha_neg * (reward - Q[action, block_idx])
            # update iteration counter
            k += 1
            
    return -llh

def RLd_LLH(param, Data): 
    """
    This function is the loglikelihood function for a simple RL model with both state and action 
    build into the decision process. Specifically, the Q function takes as input both the block 
    currently presented to the participant and the action available to them. This contains a discount factor. 
    """
    assert param.shape[0] == 3, 'Invalid amount of parameters'
    beta = param[0]
    alpha = param[1] 
    discount = param[2]
    
    n = int(np.max(Data[:, 0])) # number of participants 
    llh = 0 # log-likelihood 
    k = 0 # iteration tracker
    blocks = {"BP": 0, "BS": 1, "WP": 2, "WS": 3}
    
    for participant in range(1, n + 1): 
        # set up Q-function
        Q = np.full((2, 4), np.nan)
        Q[0, :] = 0.0
        Q[1, :] = -0.5
        for trial in Data[Data[:, 0] == participant, 1]: 
            # set trial outcomes 
            action = Data[k, 2]
            reward = Data[k, 3]
            block = Data[k, 4]
            block_idx = blocks[block]
            # ignore the first two trials for loglikelihood computation
            if trial > 2: 
                # compute softmax probabilities
                softmax = np.full(2, np.nan)
                softmax[0] = 1 / np.sum(np.exp(beta * (Q[:, block_idx] - Q[0, block_idx])))
                softmax[1] = 1 - softmax[0]
                # update the loglikelihood 
                if softmax[int(action)] == 0: 
                    softmax[int(action)] = 0.000001
                llh += np.log(softmax[int(action)])
            # update Q-value if the block was approached
            if action == 1: 
                Q[action, block_idx] += alpha * (discount * reward - Q[action, block_idx])
            # update iteration counter
            k += 1
            
    return -llh

def RL2ad_LLH(param, Data): 
    """
    This function is the loglikelihood function for a simple RL model with both state and action 
    build into the decision process. Specifically, the Q function takes as input both the block 
    currently presented to the participant and the action available to them. 
    """
    assert param.shape[0] == 4, 'Invalid amount of parameters'
    beta = param[0]
    alpha_pos = param[1] 
    alpha_neg = param[2]
    discount = param[3]
    n = int(np.max(Data[:, 0])) # number of participants 
    llh = 0 # log-likelihood 
    k = 0 # iteration tracker
    blocks = {"BP": 0, "BS": 1, "WP": 2, "WS": 3}
    
    for participant in range(1, n + 1): 
        # set up Q-function
        Q = np.full((2, 4), np.nan)
        Q[0, :] = 0.0
        Q[1, :] = -0.5
        for trial in Data[Data[:, 0] == participant, 1]: 
            # set trial outcomes 
            action = Data[k, 2]
            reward = Data[k, 3]
            block = Data[k, 4]
            block_idx = blocks[block]
            # ignore the first two trials for loglikelihood computation
            if trial > 2: 
                # compute softmax probabilities
                softmax = np.full(2, np.nan)
                softmax[0] = 1 / np.sum(np.exp(beta * (Q[:, block_idx] - Q[0, block_idx])))
                softmax[1] = 1 - softmax[0]
                # update the loglikelihood 
                if softmax[int(action)] == 0: 
                    softmax[int(action)] = 0.000001
                llh += np.log(softmax[int(action)])
            # update Q-value if the block was approached
            if action == 1: 
                if reward > 0: 
                    Q[action, block_idx] += alpha_pos * (discount * reward - Q[action, block_idx])
                else: 
                    Q[action, block_idx] += alpha_neg * (discount * reward - Q[action, block_idx])
            # update iteration counter
            k += 1
            
    return -llh

def RL2a2d_LLH(param, Data): 
    """
    This function is the loglikelihood function for a simple RL model with both state and action 
    build into the decision process. Specifically, the Q function takes as input both the block 
    currently presented to the participant and the action available to them. 
    """
    assert param.shape[0] == 5, 'Invalid amount of parameters'
    beta = param[0]
    alpha_pos = param[1] 
    alpha_neg = param[2]
    discount_pos = param[3]
    discount_neg = param[4]
    
    n = int(np.max(Data[:, 0])) # number of participants 
    llh = 0 # log-likelihood 
    k = 0 # iteration tracker
    blocks = {"BP": 0, "BS": 1, "WP": 2, "WS": 3}
    
    for participant in range(1, n + 1): 
        # set up Q-function
        Q = np.full((2, 4), np.nan)
        Q[0, :] = 0.0
        Q[1, :] = -0.5
        for trial in Data[Data[:, 0] == participant, 1]: 
            # set trial outcomes 
            action = Data[k, 2]
            reward = Data[k, 3]
            block = Data[k, 4]
            block_idx = blocks[block]
            # ignore the first two trials for loglikelihood computation
            if trial > 2: 
                # compute softmax probabilities
                softmax = np.full(2, np.nan)
                softmax[0] = 1 / np.sum(np.exp(beta * (Q[:, block_idx] - Q[0, block_idx])))
                softmax[1] = 1 - softmax[0]
                # update the loglikelihood 
                if softmax[int(action)] == 0: 
                    softmax[int(action)] = 0.000001
                llh += np.log(softmax[int(action)])
            # update Q-value if the block was approached
            if action == 1: 
                if reward > 0: 
                    Q[action, block_idx] += alpha_pos * (discount_pos * reward - Q[action, block_idx])
                else: 
                    Q[action, block_idx] += alpha_neg * (discount_neg * reward - Q[action, block_idx])
            # update iteration counter
            k += 1
            
    return -llh

def RLc_LLH(param, Data): 
    """
    This function is the loglikelihood function for a simple RL model with a "curiosity" component
    """
    assert param.shape[0] == 3, 'Invalid amount of parameters'
    beta = param[0]
    alpha = param[1] 
    curiosity = param[2]
    n = int(np.max(Data[:, 0])) # number of participants 
    llh = 0 # log-likelihood 
    k = 0 # iteration tracker
    blocks = {"BP": 0, "BS": 1, "WP": 2, "WS": 3}
    
    for participant in range(1, n + 1): 
        # set up Q-function
        Q = np.full((2, 4), np.nan)
        Q[0, :] = 0.0
        Q[1, :] = -0.5
        for trial in Data[Data[:, 0] == participant, 1]: 
            # set trial outcomes 
            block_views = np.full(4, 0)
            action = Data[k, 2]
            reward = Data[k, 3]
            block = Data[k, 4]
            block_idx = blocks[block]
            # ignore the first two trials for loglikelihood computation
            if trial > 2: 
                # compute softmax probabilities
                softmax = np.full(2, np.nan)
                softmax[0] = 1 / np.sum(np.exp(beta * (Q[:, block_idx] - Q[0, block_idx])))
                softmax[1] = 1 - softmax[0]
                # update the loglikelihood 
                if softmax[int(action)] == 0: 
                    softmax[int(action)] = 0.000001
                llh += np.log(softmax[int(action)])
            # update Q-value and block views if the block was approached
            if action == 1: 
                curiosity_bonus = curiosity / (1 + block_views[block_idx])
                Q[action, block_idx] += alpha * (reward - Q[action, block_idx]) + curiosity_bonus
                block_views[block_idx] += 1
            # update iteration counter
            k += 1
            
    return -llh

def RL2ac_LLH(param, Data): 
    """
    This function is the loglikelihood function for a simple RL model with a "curiosity" component
    """
    assert param.shape[0] == 4, 'Invalid amount of parameters'
    beta = param[0]
    alpha_positive = param[1] 
    alpha_negative = param[2]
    curiosity = param[3]
    n = int(np.max(Data[:, 0])) # number of participants 
    llh = 0 # log-likelihood 
    k = 0 # iteration tracker
    blocks = {"BP": 0, "BS": 1, "WP": 2, "WS": 3}
    
    for participant in range(1, n + 1): 
        # set up Q-function
        Q = np.full((2, 4), np.nan)
        Q[0, :] = 0.0
        Q[1, :] = -0.5
        for trial in Data[Data[:, 0] == participant, 1]: 
            # set trial outcomes 
            block_views = np.full(4, 0)
            action = Data[k, 2]
            reward = Data[k, 3]
            block = Data[k, 4]
            block_idx = blocks[block]
            # ignore the first two trials for loglikelihood computation
            if trial > 2: 
                # compute softmax probabilities
                softmax = np.full(2, np.nan)
                softmax[0] = 1 / np.sum(np.exp(beta * (Q[:, block_idx] - Q[0, block_idx])))
                softmax[1] = 1 - softmax[0]
                # update the loglikelihood 
                if softmax[int(action)] == 0: 
                    softmax[int(action)] = 0.000001
                llh += np.log(softmax[int(action)])
            # update Q-value and block views if the block was approached
            if action == 1: 
                curiosity_bonus = curiosity / (1 + block_views[block_idx])
                alpha = alpha_positive if reward > 0 else alpha_negative
                Q[action, block_idx] += alpha * (reward - Q[action, block_idx]) + curiosity_bonus
                block_views[block_idx] += 1
            # update iteration counter
            k += 1
            
    return -llh

def RLu_LLH(param, Data): 
    """
    This function is the loglikelihood function for a simple RL model with both state and action 
    build into the decision process. Specifically, the Q function takes as input both the block 
    currently presented to the participant and the action available to them. 
    """
    assert param.shape[0] == 3, 'Invalid amount of parameters'
    beta = param[0]
    alpha = param[1] 
    utility_decay = param[2]
    n = int(np.max(Data[:, 0])) # number of participants 
    llh = 0 # log-likelihood 
    k = 0 # iteration tracker
    blocks = {"BP": 0, "BS": 1, "WP": 2, "WS": 3}
    
    for participant in range(1, n + 1): 
        total_reward = 0
        # set up Q-function
        Q = np.full((2, 4), np.nan)
        Q[0, :] = 0.0
        Q[1, :] = -0.5
        for trial in Data[Data[:, 0] == participant, 1]: 
            # set trial outcomes 
            action = Data[k, 2]
            reward = Data[k, 3]
            block = Data[k, 4]
            block_idx = blocks[block]
            # ignore the first two trials for loglikelihood computation
            if trial > 2: 
                # compute softmax probabilities
                softmax = np.full(2, np.nan)
                softmax[0] = 1 / np.sum(np.exp(beta * (Q[:, block_idx] - Q[0, block_idx])))
                softmax[1] = 1 - softmax[0]
                # update the loglikelihood 
                if softmax[int(action)] == 0: 
                    softmax[int(action)] = 0.000001
                llh += np.log(softmax[int(action)])
            # update Q-value if the block was approached
            if action == 1: 
                utility = (total_reward + reward) ** utility_decay
                Q[action, block_idx] += alpha * (utility - Q[action, block_idx])
            # update iteration counter
            k += 1
            total_reward += reward
            
    return -llh

def RL_2D_LLH(param, Data): 
    """
    This function is the loglikelihood function for a RL model with two value (Q) functions. 
    """
    assert param.shape[0] == 2, 'Invalid amount of parameters'
    # parameter extraction
    beta = param[0]
    alpha = param[1] 
    n = int(np.max(Data[:, 0])) # number of participants 
    llh = 0 # log-likelihood 
    k = 0 # iteration tracker
    for participant in range(1, n + 1): 
        # Set up the Q-function for the patterns 
        Q_pattern = {}
        Q_pattern[0] = {"P": 0.0, "S": 0.0}
        Q_pattern[1] = {"P": -0.5, "S": -0.5}
        # Set up the Q-function for the colors 
        Q_color = {}
        Q_color[0] = {"B": 0.0, "W": 0.0}
        Q_color[1] = {"B": -0.5, "W": -0.5}
        for trial in Data[Data[:, 0] == participant, 1]: 
            # set trial outcomes 
            action = Data[k, 2]
            reward = Data[k, 3]
            block = Data[k, 4]
            color = block[0]
            pattern = block[1]
            # ignore the first two trials for loglikelihood computation
            if trial > 2: 
                # join the Q-values into one vector for the current block
                Q = np.full(2, np.nan)
                Q[0] = Q_color[0][color] * Q_pattern[0][pattern]
                Q[1] = Q_color[1][color] * Q_pattern[1][pattern]
                # compute softmax probability
                softmax = np.full(2, np.nan)
                softmax[0] = 1 / np.sum(np.exp(beta * (Q - Q[0])))
                softmax[1] = 1 - softmax[0]
                # update the loglikelihood 
                if softmax[int(action)] == 0: 
                    softmax[int(action)] = 0.000001
                llh += np.log(softmax[int(action)])
            # update Q-value if the block was approached
            if action == 1: 
                Q_color[action][color] += alpha * (reward - Q_color[action][color])
                Q_pattern[action][pattern] += alpha * (reward - Q_pattern[action][pattern])
            # update iteration counter
            k += 1
            
    return -llh

def RL_2D2a_LLH(param, Data): 
    """
    This function is the loglikelihood function for a RL model with two value (Q) functions. 
    """
    assert param.shape[0] == 3, 'Invalid amount of parameters'
    # parameter extraction
    beta = param[0]
    alpha_color = param[1] 
    alpha_pattern = param[2]

    n = int(np.max(Data[:, 0])) # number of participants 
    llh = 0 # log-likelihood 
    k = 0 # iteration tracker
    for participant in range(1, n + 1): 
        # Set up the Q-function for the patterns 
        Q_pattern = {}
        Q_pattern[0] = {"P": 0.0, "S": 0.0}
        Q_pattern[1] = {"P": -0.5, "S": -0.5}
        # Set up the Q-function for the colors 
        Q_color = {}
        Q_color[0] = {"B": 0.0, "W": 0.0}
        Q_color[1] = {"B": -0.5, "W": -0.5}
        for trial in Data[Data[:, 0] == participant, 1]: 
            # set trial outcomes 
            action = Data[k, 2]
            reward = Data[k, 3]
            block = Data[k, 4]
            color = block[0]
            pattern = block[1]
            # ignore the first two trials for loglikelihood computation
            if trial > 2: 
                # join the Q-values into one vector for the current block
                Q = np.full(2, np.nan)
                Q[0] = Q_color[0][color] * Q_pattern[0][pattern]
                Q[1] = Q_color[1][color] * Q_pattern[1][pattern]
                # compute softmax probability
                softmax = np.full(2, np.nan)
                softmax[0] = 1 / np.sum(np.exp(beta * (Q - Q[0])))
                softmax[1] = 1 - softmax[0]
                # update the loglikelihood 
                if softmax[int(action)] == 0: 
                    softmax[int(action)] = 0.000001
                llh += np.log(softmax[int(action)])
            # update Q-value if the block was approached
            if action == 1: 
                Q_color[action][color] += alpha_color * (reward - Q_color[action][color])
                Q_pattern[action][pattern] += alpha_pattern * (reward - Q_pattern[action][pattern])
            # update iteration counter
            k += 1
            
    return -llh

def RL2a_2D_LLH(param, Data): 
    """
    This function is the loglikelihood function for a RL model with two value (Q) functions. 
    """
    assert param.shape[0] == 3, 'Invalid amount of parameters'
    # parameter extraction
    beta = param[0]
    alpha_positive = param[1] 
    alpha_negative = param[2]

    n = int(np.max(Data[:, 0])) # number of participants 
    llh = 0 # log-likelihood 
    k = 0 # iteration tracker
    for participant in range(1, n + 1): 
        # Set up the Q-function for the patterns 
        Q_pattern = {}
        Q_pattern[0] = {"P": 0.0, "S": 0.0}
        Q_pattern[1] = {"P": -0.5, "S": -0.5}
        # Set up the Q-function for the colors 
        Q_color = {}
        Q_color[0] = {"B": 0.0, "W": 0.0}
        Q_color[1] = {"B": -0.5, "W": -0.5}
        for trial in Data[Data[:, 0] == participant, 1]: 
            # set trial outcomes 
            action = Data[k, 2]
            reward = Data[k, 3]
            block = Data[k, 4]
            color = block[0]
            pattern = block[1]
            # ignore the first two trials for loglikelihood computation
            if trial > 2: 
                # join the Q-values into one vector for the current block
                Q = np.full(2, np.nan)
                Q[0] = Q_color[0][color] * Q_pattern[0][pattern]
                Q[1] = Q_color[1][color] * Q_pattern[1][pattern]
                # compute softmax probability
                softmax = np.full(2, np.nan)
                softmax[0] = 1 / np.sum(np.exp(beta * (Q - Q[0])))
                softmax[1] = 1 - softmax[0]
                # update the loglikelihood 
                if softmax[int(action)] == 0: 
                    softmax[int(action)] = 0.000001
                llh += np.log(softmax[int(action)])
            # update Q-value if the block was approached
            if action == 1: 
                if reward > 0: 
                    alpha = alpha_positive
                else: 
                    alpha = alpha_negative 
                Q_color[action][color] += alpha * (reward - Q_color[action][color])
                Q_pattern[action][pattern] += alpha * (reward - Q_pattern[action][pattern])
            # update iteration counter
            k += 1
            
    return -llh

def RL2a_2D2a_LLH(param, Data): 
    """
    This function is the loglikelihood function for a RL model with two value (Q) functions. 
    """
    assert param.shape[0] == 5, 'Invalid amount of parameters'
    # parameter extraction
    beta = param[0]
    alpha_color_positive = param[1] 
    alpha_color_negative = param[2] 
    alpha_pattern_positive = param[3]
    alpha_pattern_negative = param[4]

    n = int(np.max(Data[:, 0])) # number of participants 
    llh = 0 # log-likelihood 
    k = 0 # iteration tracker
    for participant in range(1, n + 1): 
        # Set up the Q-function for the patterns 
        Q_pattern = {}
        Q_pattern[0] = {"P": 0.0, "S": 0.0}
        Q_pattern[1] = {"P": -0.5, "S": -0.5}
        # Set up the Q-function for the colors 
        Q_color = {}
        Q_color[0] = {"B": 0.0, "W": 0.0}
        Q_color[1] = {"B": -0.5, "W": -0.5}
        for trial in Data[Data[:, 0] == participant, 1]: 
            # set trial outcomes 
            action = Data[k, 2]
            reward = Data[k, 3]
            block = Data[k, 4]
            color = block[0]
            pattern = block[1]
            # ignore the first two trials for loglikelihood computation
            if trial > 2: 
                # join the Q-values into one vector for the current block
                Q = np.full(2, np.nan)
                Q[0] = Q_color[0][color] * Q_pattern[0][pattern]
                Q[1] = Q_color[1][color] * Q_pattern[1][pattern]
                # compute softmax probability
                softmax = np.full(2, np.nan)
                softmax[0] = 1 / np.sum(np.exp(beta * (Q - Q[0])))
                softmax[1] = 1 - softmax[0]
                # update the loglikelihood 
                if softmax[int(action)] == 0: 
                    softmax[int(action)] = 0.000001
                llh += np.log(softmax[int(action)])
            # update Q-value if the block was approached
            if action == 1: 
                alpha_color = alpha_color_positive if reward > 0 else alpha_color_negative
                alpha_pattern = alpha_pattern_positive if reward > 0 else alpha_pattern_negative
                Q_color[action][color] += alpha_color * (reward - Q_color[action][color])
                Q_pattern[action][pattern] += alpha_pattern * (reward - Q_pattern[action][pattern])
            # update iteration counter
            k += 1
            
    return -llh

def RLc_2D_LLH(param, Data): 
    """
    This function is the loglikelihood function for a RL model with two value (Q) functions. 
    """
    assert param.shape[0] == 3, 'Invalid amount of parameters'
    # parameter extraction
    beta = param[0]
    alpha = param[1] 
    curiosity = param[2]
    block_to_idx = {"BP": 0, "BS": 1, "WP": 2, "WS": 3}
    n = int(np.max(Data[:, 0])) # number of participants 
    llh = 0 # log-likelihood 
    k = 0 # iteration tracker
    for participant in range(1, n + 1): 
        block_views = np.full(4, 0)
        # Set up the Q-function for the patterns 
        Q_pattern = {}
        Q_pattern[0] = {"P": 0.0, "S": 0.0}
        Q_pattern[1] = {"P": -0.5, "S": -0.5}
        # Set up the Q-function for the colors 
        Q_color = {}
        Q_color[0] = {"B": 0.0, "W": 0.0}
        Q_color[1] = {"B": -0.5, "W": -0.5}
        for trial in Data[Data[:, 0] == participant, 1]: 
            # set trial outcomes 
            action = Data[k, 2]
            reward = Data[k, 3]
            block = Data[k, 4]
            color = block[0]
            pattern = block[1]
            block_idx = block_to_idx[block]
            # ignore the first two trials for loglikelihood computation
            if trial > 2: 
                # join the Q-values into one vector for the current block
                Q = np.full(2, np.nan)
                Q[0] = Q_color[0][color] * Q_pattern[0][pattern]
                Q[1] = Q_color[1][color] * Q_pattern[1][pattern]
                # compute softmax probability
                softmax = np.full(2, np.nan)
                softmax[0] = 1 / np.sum(np.exp(beta * (Q - Q[0])))
                softmax[1] = 1 - softmax[0]
                # update the loglikelihood 
                if softmax[int(action)] == 0: 
                    softmax[int(action)] = 0.000001
                llh += np.log(softmax[int(action)])
            # update Q-value if the block was approached
            if action == 1: 
                curiosity_bonus = curiosity / (1 + block_views[block_idx])
                Q_color[action][color] += alpha * (reward - Q_color[action][color]) + curiosity_bonus
                Q_pattern[action][pattern] += alpha * (reward - Q_pattern[action][pattern]) + curiosity_bonus
                block_views[block_idx] += 1
            # update iteration counter
            k += 1
            
    return -llh

def RL2ac_2D_LLH(param, Data): 
    """
    This function is the loglikelihood function for a RL model with two value (Q) functions. 
    """
    assert param.shape[0] == 4, 'Invalid amount of parameters'
    # parameter extraction
    beta = param[0]
    alpha_positive = param[1] 
    alpha_negative = param[2]
    curiosity = param[3]
    block_to_idx = {"BP": 0, "BS": 1, "WP": 2, "WS": 3}
    n = int(np.max(Data[:, 0])) # number of participants 
    llh = 0 # log-likelihood 
    k = 0 # iteration tracker
    for participant in range(1, n + 1): 
        block_views = np.full(4, 0)
        # Set up the Q-function for the patterns 
        Q_pattern = {}
        Q_pattern[0] = {"P": 0.0, "S": 0.0}
        Q_pattern[1] = {"P": -0.5, "S": -0.5}
        # Set up the Q-function for the colors 
        Q_color = {}
        Q_color[0] = {"B": 0.0, "W": 0.0}
        Q_color[1] = {"B": -0.5, "W": -0.5}
        for trial in Data[Data[:, 0] == participant, 1]: 
            # set trial outcomes 
            action = Data[k, 2]
            reward = Data[k, 3]
            block = Data[k, 4]
            color = block[0]
            pattern = block[1]
            block_idx = block_to_idx[block]
            # ignore the first two trials for loglikelihood computation
            if trial > 2: 
                # join the Q-values into one vector for the current block
                Q = np.full(2, np.nan)
                Q[0] = Q_color[0][color] * Q_pattern[0][pattern]
                Q[1] = Q_color[1][color] * Q_pattern[1][pattern]
                # compute softmax probability
                softmax = np.full(2, np.nan)
                softmax[0] = 1 / np.sum(np.exp(beta * (Q - Q[0])))
                softmax[1] = 1 - softmax[0]
                # update the loglikelihood 
                if softmax[int(action)] == 0: 
                    softmax[int(action)] = 0.000001
                llh += np.log(softmax[int(action)])
            # update Q-value if the block was approached
            if action == 1: 
                curiosity_bonus = curiosity / (1 + block_views[block_idx])
                alpha = alpha_positive if reward > 0 else alpha_negative
                Q_color[action][color] += alpha * (reward - Q_color[action][color]) + curiosity_bonus
                Q_pattern[action][pattern] += alpha * (reward - Q_pattern[action][pattern]) + curiosity_bonus
                block_views[block_idx] += 1
            # update iteration counter
            k += 1
            
    return -llh

def RLc_2D2a_LLH(param, Data): 
    """
    This function is the loglikelihood function for a RL model with two value (Q) functions. 
    """
    assert param.shape[0] == 4, 'Invalid amount of parameters'
    # parameter extraction
    beta = param[0]
    alpha_color = param[1] 
    alpha_pattern = param[2]
    curiosity = param[3]
    block_to_idx = {"BP": 0, "BS": 1, "WP": 2, "WS": 3}
    n = int(np.max(Data[:, 0])) # number of participants 
    llh = 0 # log-likelihood 
    k = 0 # iteration tracker
    for participant in range(1, n + 1): 
        block_views = np.full(4, 0)
        # Set up the Q-function for the patterns 
        Q_pattern = {}
        Q_pattern[0] = {"P": 0.0, "S": 0.0}
        Q_pattern[1] = {"P": -0.5, "S": -0.5}
        # Set up the Q-function for the colors 
        Q_color = {}
        Q_color[0] = {"B": 0.0, "W": 0.0}
        Q_color[1] = {"B": -0.5, "W": -0.5}
        for trial in Data[Data[:, 0] == participant, 1]: 
            # set trial outcomes 
            action = Data[k, 2]
            reward = Data[k, 3]
            block = Data[k, 4]
            color = block[0]
            pattern = block[1]
            block_idx = block_to_idx[block]
            # ignore the first two trials for loglikelihood computation
            if trial > 2: 
                # join the Q-values into one vector for the current block
                Q = np.full(2, np.nan)
                Q[0] = Q_color[0][color] * Q_pattern[0][pattern]
                Q[1] = Q_color[1][color] * Q_pattern[1][pattern]
                # compute softmax probability
                softmax = np.full(2, np.nan)
                softmax[0] = 1 / np.sum(np.exp(beta * (Q - Q[0])))
                softmax[1] = 1 - softmax[0]
                # update the loglikelihood 
                if softmax[int(action)] == 0: 
                    softmax[int(action)] = 0.000001
                llh += np.log(softmax[int(action)])
            # update Q-value if the block was approached
            if action == 1: 
                curiosity_bonus = curiosity / (1 + block_views[block_idx])
                Q_color[action][color] += alpha_color * (reward - Q_color[action][color]) + curiosity_bonus
                Q_pattern[action][pattern] += alpha_pattern * (reward - Q_pattern[action][pattern]) + curiosity_bonus
                block_views[block_idx] += 1
            # update iteration counter
            k += 1
            
    return -llh

def RL2ac_2D2a_LLH(param, Data): 
    """
    This function is the loglikelihood function for a RL model with two value (Q) functions. 
    """
    assert param.shape[0] == 6, 'Invalid amount of parameters'
    # parameter extraction
    beta = param[0]
    alpha_color_positive = param[1] 
    alpha_color_negative = param[2]
    alpha_pattern_positive = param[3]
    alpha_pattern_negative = param[4]
    curiosity = param[5]
    block_to_idx = {"BP": 0, "BS": 1, "WP": 2, "WS": 3}
    n = int(np.max(Data[:, 0])) # number of participants 
    llh = 0 # log-likelihood 
    k = 0 # iteration tracker
    for participant in range(1, n + 1): 
        block_views = np.full(4, 0)
        # Set up the Q-function for the patterns 
        Q_pattern = {}
        Q_pattern[0] = {"P": 0.0, "S": 0.0}
        Q_pattern[1] = {"P": -0.5, "S": -0.5}
        # Set up the Q-function for the colors 
        Q_color = {}
        Q_color[0] = {"B": 0.0, "W": 0.0}
        Q_color[1] = {"B": -0.5, "W": -0.5}
        for trial in Data[Data[:, 0] == participant, 1]: 
            # set trial outcomes 
            action = Data[k, 2]
            reward = Data[k, 3]
            block = Data[k, 4]
            color = block[0]
            pattern = block[1]
            block_idx = block_to_idx[block]
            # ignore the first two trials for loglikelihood computation
            if trial > 2: 
                # join the Q-values into one vector for the current block
                Q = np.full(2, np.nan)
                Q[0] = Q_color[0][color] * Q_pattern[0][pattern]
                Q[1] = Q_color[1][color] * Q_pattern[1][pattern]
                # compute softmax probability
                softmax = np.full(2, np.nan)
                softmax[0] = 1 / np.sum(np.exp(beta * (Q - Q[0])))
                softmax[1] = 1 - softmax[0]
                # update the loglikelihood 
                if softmax[int(action)] == 0: 
                    softmax[int(action)] = 0.000001
                llh += np.log(softmax[int(action)])
            # update Q-value if the block was approached
            if action == 1: 
                curiosity_bonus = curiosity / (1 + block_views[block_idx])
                alpha_color = alpha_color_positive if reward > 0 else alpha_color_negative
                Q_color[action][color] += alpha_color * (reward - Q_color[action][color]) + curiosity_bonus
                alpha_pattern = alpha_pattern_positive if reward > 0 else alpha_pattern_negative
                Q_pattern[action][pattern] += alpha_pattern * (reward - Q_pattern[action][pattern]) + curiosity_bonus
                block_views[block_idx] += 1
            # update iteration counter
            k += 1
            
    return -llh



