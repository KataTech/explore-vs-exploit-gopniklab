import numpy as np 

def BaseLine(param, numParticipants, trialSets): 
    """
    Generate simulated data with a baseline model, i.e. random decisions. 
    """
    Data = np.ones(5)
    blocks = np.array(["BP", "BS", "WP", "WS"])
    rewards = np.array([-2.0, 1.0])
    
    for participant in range(1, numParticipants + 1): 
        trial = 1
        bad_block_idx = np.random.choice(4) # select a bad block for the current participant
        # first trial set - the first two blocks are shown and the second must be non-zaff
        good_block_idx = np.delete(np.random.permutation(4), bad_block_idx)
        for i in range(4): 
            if i == 0: 
                block_idx = good_block_idx[i]; action = 1; reward = rewards[1];
            elif i == 1: 
                block_idx = bad_block_idx; action = 1; reward = rewards[0]; 
            else: 
                block_idx = good_block_idx[i - 1]
                action = np.random.choice(range(2))
                reward = 0 if action == 0 else rewards[int(block_idx != bad_block_idx)]
            Data = np.vstack((Data, np.array([participant, trial, action, reward, blocks[block_idx]], dtype = object)))
            trial += 1
        # subsequent trial set 
            for trialset in range(2, trialSets + 1): 
                for block_idx in np.random.permutation(4): 
                    # determine approach vs. avoid 
                    action = np.random.choice(range(2))
                    # update values depending on action
                    reward = 0 if action == 0 else rewards[int(block_idx != bad_block_idx)]
                    # store the data
                    Data = np.vstack((Data, np.array([participant, trial, action, reward, blocks[block_idx]], dtype = object)))
                    trial += 1
    return Data[1:]  # remove first row
         

def RL(param, numParticipants, trialSets): 
    """
    Generates simulated data with the two-factor Q-learning model, i.e. the Q function 
    takes as input both the block currently presented to the participant and the action 
    available to them overtime. This is an extension/modification of the RL function. 
    """
    # set-up
    beta = param[0]
    alpha = param[1]
    blocks = np.array(["BP", "BS", "WP", "WS"])
    rewards = np.array([-2.0, 1.0])
    Data = np.ones(5)
        
    for participant in range(1, numParticipants + 1): 
        trial = 1 # counter for the trial number
        bad_block_idx = np.random.choice(4) # select a bad block for the current participant
        Q = np.full((2, 4), 0.5) # set up Q-function
        Q[0, :] = 0.0
        Q[1, :] = -0.5
        # first trial set - the first two blocks are shown and the second must be non-zaff
        good_block_idx = np.delete(np.random.permutation(4), bad_block_idx)
        for i in range(4): 
            if i == 0: 
                block_idx = good_block_idx[i]; action = 1; reward = rewards[1];
                Q[action, block_idx] += alpha * (reward - Q[action, block_idx])
            elif i == 1: 
                block_idx = bad_block_idx; action = 1; reward = rewards[0]; 
                Q[action, block_idx] += alpha * (reward - Q[action, block_idx])
            else: 
                block_idx = good_block_idx[i - 1]
                softmax = np.array([0.5, 0.5])
                softmax[0] = 1 / np.sum(np.exp(beta * (Q[:, block_idx] - Q[0, block_idx])))
                softmax[1] = 1 - softmax[0]
                action = np.random.choice(range(2), p=softmax)
                if action == 0: # avoid
                    reward = 0 
                else: # approach
                    reward = rewards[int(block_idx != bad_block_idx)]
                    Q[action, block_idx] += alpha * (reward - Q[action, block_idx])
            Data = np.vstack((Data, 
                              np.array([participant, trial, action, reward, blocks[block_idx]], dtype = object)))
            trial += 1
        # subsequent trial set 
        for trialset in range(2, trialSets + 1): 
            for block_idx in np.random.permutation(4): 
                # determine approach vs. avoid - bernoulli trial
                softmax = np.array([0.5, 0.5])
                softmax[0] = 1 / np.sum(np.exp(beta * (Q[:, block_idx] - Q[0, block_idx])))
                softmax[1] = 1 - softmax[0]
                action = np.random.choice(range(2), p=softmax)
                # update values depending on action
                if action == 0: # avoid
                    reward = 0 
                else: # approach
                    reward = rewards[int(block_idx != bad_block_idx)]
                    Q[action, block_idx] += alpha * (reward - Q[action, block_idx])
                # store the data
                Data = np.vstack((Data, 
                                  np.array([participant, trial, action, reward, blocks[block_idx]], dtype = object)))
                trial += 1
                
    return Data[1:]  # remove first row

def RL2a(param, numParticipants, trialSets): 
    """
    Generates simulated data with the two-factor Q-learning model, i.e. the Q function 
    takes as input both the block currently presented to the participant and the action 
    available to them overtime. This is an extension/modification of the RL function. 
    """
    # set-up
    beta = param[0]
    alpha_pos = param[1]
    alpha_neg = param[2]
    blocks = np.array(["BP", "BS", "WP", "WS"])
    rewards = np.array([-2.0, 1.0])
    Data = np.ones(5)
        
    for participant in range(1, numParticipants + 1): 
        trial = 1 # counter for the trial number
        bad_block_idx = np.random.choice(4) # select a bad block for the current participant
        Q = np.full((2, 4), 0.5) # set up Q-function
        Q[0, :] = 0.0
        Q[1, :] = -0.5
        # first trial set - the first two blocks are shown and the second must be non-zaff
        good_block_idx = np.delete(np.random.permutation(4), bad_block_idx)
        for i in range(4): 
            if i == 0: 
                block_idx = good_block_idx[i]; action = 1; reward = rewards[1];
                Q[action, block_idx] += alpha_pos * (reward - Q[action, block_idx])
            elif i == 1: 
                block_idx = bad_block_idx; action = 1; reward = rewards[0]; 
                Q[action, block_idx] += alpha_neg * (reward - Q[action, block_idx])
            else: 
                block_idx = good_block_idx[i - 1]
                softmax = np.array([0.5, 0.5])
                softmax[0] = 1 / np.sum(np.exp(beta * (Q[:, block_idx] - Q[0, block_idx])))
                softmax[1] = 1 - softmax[0]
                action = np.random.choice(range(2), p=softmax)
                if action == 0: # avoid
                    reward = 0 
                else: # approach
                    reward = rewards[int(block_idx != bad_block_idx)]
                    Q[action, block_idx] += alpha_pos * (reward - Q[action, block_idx])
            Data = np.vstack((Data, 
                              np.array([participant, trial, action, reward, blocks[block_idx]], dtype = object)))
            trial += 1
        # subsequent trial set 
        for trialset in range(2, trialSets + 1): 
            for block_idx in np.random.permutation(4): 
                # determine approach vs. avoid - bernoulli trial
                softmax = np.array([0.5, 0.5])
                softmax[0] = 1 / np.sum(np.exp(beta * (Q[:, block_idx] - Q[0, block_idx])))
                softmax[1] = 1 - softmax[0]
                action = np.random.choice(range(2), p=softmax)
                # update values depending on action
                if action == 0: # avoid
                    reward = 0 
                else: # approach
                    reward = rewards[int(block_idx != bad_block_idx)]
                    if reward > 0: 
                        Q[action, block_idx] += alpha_pos * (reward - Q[action, block_idx])
                    else: 
                        Q[action, block_idx] += alpha_neg * (reward - Q[action, block_idx])
                # store the data
                Data = np.vstack((Data, 
                                  np.array([participant, trial, action, reward, blocks[block_idx]], dtype = object)))
                trial += 1
                
    return Data[1:]  # remove first row

def RLd(param, numParticipants, trialSets): 
    """
    Generates simulated data with the two-factor Q-learning model, i.e. the Q function 
    takes as input both the block currently presented to the participant and the action 
    available to them overtime. This is an extension/modification of the RL function. 
    """
    # set-up
    beta = param[0]
    alpha = param[1]
    discount = param[2]
    blocks = np.array(["BP", "BS", "WP", "WS"])
    rewards = np.array([-2.0, 1.0])
    Data = np.ones(5)
        
    for participant in range(1, numParticipants + 1): 
        trial = 1 # counter for the trial number
        bad_block_idx = np.random.choice(4) # select a bad block for the current participant
        Q = np.full((2, 4), 0.5) # set up Q-function
        Q[0, :] = 0.0
        Q[1, :] = -0.5
        # first trial set - the first two blocks are shown and the second must be non-zaff
        good_block_idx = np.delete(np.random.permutation(4), bad_block_idx)
        for i in range(4): 
            if i == 0: 
                block_idx = good_block_idx[i]; action = 1; reward = rewards[1];
                Q[action, block_idx] += alpha * (discount * reward - Q[action, block_idx])
            elif i == 1: 
                block_idx = bad_block_idx; action = 1; reward = rewards[0]; 
                Q[action, block_idx] += alpha * (discount * reward - Q[action, block_idx])
            else: 
                block_idx = good_block_idx[i - 1]
                softmax = np.array([0.5, 0.5])
                softmax[0] = 1 / np.sum(np.exp(beta * (Q[:, block_idx] - Q[0, block_idx])))
                softmax[1] = 1 - softmax[0]
                action = np.random.choice(range(2), p=softmax)
                if action == 0: # avoid
                    reward = 0 
                else: # approach
                    reward = rewards[int(block_idx != bad_block_idx)]
                    Q[action, block_idx] += alpha * (discount * reward - Q[action, block_idx])
            Data = np.vstack((Data, 
                              np.array([participant, trial, action, reward, blocks[block_idx]], dtype = object)))
            trial += 1
        # subsequent trial set 
        for trialset in range(2, trialSets + 1): 
            for block_idx in np.random.permutation(4): 
                # determine approach vs. avoid - bernoulli trial
                softmax = np.array([0.5, 0.5])
                softmax[0] = 1 / np.sum(np.exp(beta * (Q[:, block_idx] - Q[0, block_idx])))
                softmax[1] = 1 - softmax[0]
                action = np.random.choice(range(2), p=softmax)
                # update values depending on action
                if action == 0: # avoid
                    reward = 0 
                else: # approach
                    reward = rewards[int(block_idx != bad_block_idx)]
                    Q[action, block_idx] += alpha * (discount * reward - Q[action, block_idx])
                # store the data
                Data = np.vstack((Data, 
                                  np.array([participant, trial, action, reward, blocks[block_idx]], dtype = object)))
                trial += 1
                
    return Data[1:]  # remove first row

def RL2ad(param, numParticipants, trialSets): 
    """
    Generates simulated data with the two-factor Q-learning model, i.e. the Q function 
    takes as input both the block currently presented to the participant and the action 
    available to them overtime. Additionally, introduce a "discount factor" to the 
    policy update process. 
    
    This is an extension/modification of the RL2a function. 
    """
    # set-up
    beta = param[0]
    alpha_pos = param[1]
    alpha_neg = param[2]
    discount = param[3]
    blocks = np.array(["BP", "BS", "WP", "WS"])
    rewards = np.array([-2.0, 1.0])
    Data = np.ones(5)
        
    for participant in range(1, numParticipants + 1): 
        trial = 1 # counter for the trial number
        bad_block_idx = np.random.choice(4) # select a bad block for the current participant
        Q = np.full((2, 4), 0.5) # set up Q-function
        Q[0, :] = 0.0
        Q[1, :] = -0.5
        # first trial set - the first two blocks are shown and the second must be non-zaff
        good_block_idx = np.delete(np.random.permutation(4), bad_block_idx)
        for i in range(4): 
            if i == 0: 
                block_idx = good_block_idx[i]; action = 1; reward = rewards[1];
                Q[action, block_idx] += alpha_pos * (discount * reward - Q[action, block_idx])
            elif i == 1: 
                block_idx = bad_block_idx; action = 1; reward = rewards[0]; 
                Q[action, block_idx] += alpha_neg * (discount * reward - Q[action, block_idx])
            else: 
                block_idx = good_block_idx[i - 1]
                softmax = np.array([0.5, 0.5])
                softmax[0] = 1 / np.sum(np.exp(beta * (Q[:, block_idx] - Q[0, block_idx])))
                softmax[1] = 1 - softmax[0]
                action = np.random.choice(range(2), p=softmax)
                if action == 0: # avoid
                    reward = 0 
                else: # approach
                    reward = rewards[int(block_idx != bad_block_idx)]
                    Q[action, block_idx] += alpha_pos * (discount * reward - Q[action, block_idx])
            Data = np.vstack((Data, 
                              np.array([participant, trial, action, reward, blocks[block_idx]], dtype = object)))
            trial += 1
        # subsequent trial set 
        for trialset in range(2, trialSets + 1): 
            for block_idx in np.random.permutation(4): 
                # determine approach vs. avoid - bernoulli trial
                softmax = np.array([0.5, 0.5])
                softmax[0] = 1 / np.sum(np.exp(beta * (Q[:, block_idx] - Q[0, block_idx])))
                softmax[1] = 1 - softmax[0]
                action = np.random.choice(range(2), p=softmax)
                # update values depending on action
                if action == 0: # avoid
                    reward = 0 
                else: # approach
                    reward = rewards[int(block_idx != bad_block_idx)]
                    if reward > 0: 
                        Q[action, block_idx] += alpha_pos * (discount * reward - Q[action, block_idx])
                    else: 
                        Q[action, block_idx] += alpha_neg * (discount * reward - Q[action, block_idx])
                # store the data
                Data = np.vstack((Data, 
                                  np.array([participant, trial, action, reward, blocks[block_idx]], dtype = object)))
                trial += 1
                
    return Data[1:]  # remove first row

def RL2a2d(param, numParticipants, trialSets): 
    """
    Generates simulated data with the two-factor Q-learning model, i.e. the Q function 
    takes as input both the block currently presented to the participant and the action 
    available to them overtime. Additionally, introduce a "discount factor" to the 
    policy update process. 
    
    This is an extension/modification of the RL2a function. 
    """
    # set-up
    beta = param[0]
    alpha_pos = param[1]
    alpha_neg = param[2]
    discount_pos = param[3]
    discount_neg = param[4]
    blocks = np.array(["BP", "BS", "WP", "WS"])
    rewards = np.array([-2.0, 1.0])
    Data = np.ones(5)
        
    for participant in range(1, numParticipants + 1): 
        trial = 1 # counter for the trial number
        bad_block_idx = np.random.choice(4) # select a bad block for the current participant
        Q = np.full((2, 4), 0.5) # set up Q-function
        Q[0, :] = 0.0
        Q[1, :] = -0.5
        # first trial set - the first two blocks are shown and the second must be non-zaff
        good_block_idx = np.delete(np.random.permutation(4), bad_block_idx)
        for i in range(4): 
            if i == 0: 
                block_idx = good_block_idx[i]; action = 1; reward = rewards[1];
                Q[action, block_idx] += alpha_pos * (discount_pos * reward - Q[action, block_idx])
            elif i == 1: 
                block_idx = bad_block_idx; action = 1; reward = rewards[0]; 
                Q[action, block_idx] += alpha_neg * (discount_neg * reward - Q[action, block_idx])
            else: 
                block_idx = good_block_idx[i - 1]
                softmax = np.array([0.5, 0.5])
                softmax[0] = 1 / np.sum(np.exp(beta * (Q[:, block_idx] - Q[0, block_idx])))
                softmax[1] = 1 - softmax[0]
                action = np.random.choice(range(2), p=softmax)
                if action == 0: # avoid
                    reward = 0 
                else: # approach
                    reward = rewards[int(block_idx != bad_block_idx)]
                    Q[action, block_idx] += alpha_pos * (discount_pos * reward - Q[action, block_idx])
            Data = np.vstack((Data, 
                              np.array([participant, trial, action, reward, blocks[block_idx]], dtype = object)))
            trial += 1
        # subsequent trial set 
        for trialset in range(2, trialSets + 1): 
            for block_idx in np.random.permutation(4): 
                # determine approach vs. avoid - bernoulli trial
                softmax = np.array([0.5, 0.5])
                softmax[0] = 1 / np.sum(np.exp(beta * (Q[:, block_idx] - Q[0, block_idx])))
                softmax[1] = 1 - softmax[0]
                action = np.random.choice(range(2), p=softmax)
                # update values depending on action
                if action == 0: # avoid
                    reward = 0 
                else: # approach
                    reward = rewards[int(block_idx != bad_block_idx)]
                    if reward > 0: 
                        Q[action, block_idx] += alpha_pos * (discount_pos * reward - Q[action, block_idx])
                    else: 
                        Q[action, block_idx] += alpha_neg * (discount_neg * reward - Q[action, block_idx])
                # store the data
                Data = np.vstack((Data, 
                                  np.array([participant, trial, action, reward, blocks[block_idx]], dtype = object)))
                trial += 1
                
    return Data[1:]  # remove first row

def RLc(param, numParticipants, trialSets): 
    """
    Generates simulated data with the simple reinforcement learning model with a built-in curiosity component
    
    The exact implementation of the curiosity component lies fully in the reward, i.e. there is an additional
    scaling on a fixed reward for approaching never visited objects. Note that this form is limiting in many
    aspects, with the primary one being that the initial decision fails to be captured by the curiosity component. 
    """
    # set-up
    beta = param[0]
    alpha = param[1]
    curiosity = param[2]
    blocks = np.array(["BP", "BS", "WP", "WS"])
    rewards = np.array([-2.0, 1.0])
    Data = np.ones(5)
        
    for participant in range(1, numParticipants + 1): 
        block_views = np.full(4, 0)
        trial = 1 # counter for the trial number
        bad_block_idx = np.random.choice(4) # select a bad block for the current participant
        Q = np.full((2, 4), 0.5) # set up Q-function
        Q[0, :] = 0.0
        Q[1, :] = -0.5
        # first trial set - the first two blocks are shown and the second must be non-zaff
        good_block_idx = np.delete(np.random.permutation(4), bad_block_idx)
        for i in range(4): 
            if i == 0: 
                block_idx = good_block_idx[i]; action = 1; reward = rewards[1];
                curiosity_bonus = curiosity / (1 + block_views[block_idx])
                Q[action, block_idx] += alpha * (reward - Q[action, block_idx]) + curiosity_bonus
                block_views[block_idx] += 1
            elif i == 1: 
                block_idx = bad_block_idx; action = 1; reward = rewards[0]; 
                curiosity_bonus = curiosity / (1 + block_views[block_idx])
                Q[action, block_idx] += alpha * (reward - Q[action, block_idx]) + curiosity_bonus
                block_views[block_idx] += 1
            else: 
                block_idx = good_block_idx[i - 1]
                softmax = np.array([0.5, 0.5])
                softmax[0] = 1 / np.sum(np.exp(beta * (Q[:, block_idx] - Q[0, block_idx])))
                softmax[1] = 1 - softmax[0]
                action = np.random.choice(range(2), p=softmax)
                if action == 0: # avoid
                    reward = 0 
                else: # approach
                    reward = rewards[int(block_idx != bad_block_idx)]
                    curiosity_bonus = curiosity / (1 + block_views[block_idx])
                    Q[action, block_idx] += alpha * (reward - Q[action, block_idx]) + curiosity_bonus
                    block_views[block_idx] += 1
            Data = np.vstack((Data, 
                              np.array([participant, trial, action, reward, blocks[block_idx]], dtype = object)))
            trial += 1
        # subsequent trial set 
        for trialset in range(2, trialSets + 1): 
            for block_idx in np.random.permutation(4): 
                # determine approach vs. avoid - bernoulli trial
                softmax = np.array([0.5, 0.5])
                softmax[0] = 1 / np.sum(np.exp(beta * (Q[:, block_idx] - Q[0, block_idx])))
                softmax[1] = 1 - softmax[0]
                action = np.random.choice(range(2), p=softmax)
                # update values depending on action
                if action == 0: # avoid
                    reward = 0 
                else: # approach
                    reward = rewards[int(block_idx != bad_block_idx)]
                    # compute the curiosity bonus with the number of encounter with the block and decay rate as input
                    curiosity_bonus = curiosity / (1 + block_views[block_idx])
                    Q[action, block_idx] += alpha * (reward - Q[action, block_idx]) + curiosity_bonus
                    block_views[block_idx] += 1
                # store the data
                Data = np.vstack((Data, 
                                  np.array([participant, trial, action, reward, blocks[block_idx]], dtype = object)))
                trial += 1
                
    return Data[1:]  # remove first row

def RL2ac(param, numParticipants, trialSets): 
    """
    Generates simulated data with the simple reinforcement learning model with a built-in curiosity component
    
    The exact implementation of the curiosity component lies fully in the reward, i.e. there is an additional
    scaling on a fixed reward for approaching never visited objects. Note that this form is limiting in many
    aspects, with the primary one being that the initial decision fails to be captured by the curiosity component. 
    """
    # set-up
    beta = param[0]
    alpha_positive = param[1]
    alpha_negative = param[2]
    curiosity = param[3]
    blocks = np.array(["BP", "BS", "WP", "WS"])
    rewards = np.array([-2.0, 1.0])
    Data = np.ones(5)
        
    for participant in range(1, numParticipants + 1): 
        block_views = np.full(4, 0)
        trial = 1 # counter for the trial number
        bad_block_idx = np.random.choice(4) # select a bad block for the current participant
        Q = np.full((2, 4), 0.5) # set up Q-function
        Q[0, :] = 0.0
        Q[1, :] = -0.5
        # first trial set - the first two blocks are shown and the second must be non-zaff
        good_block_idx = np.delete(np.random.permutation(4), bad_block_idx)
        for i in range(4): 
            if i == 0: 
                block_idx = good_block_idx[i]; action = 1; reward = rewards[1];
                curiosity_bonus = curiosity / (1 + block_views[block_idx])
                Q[action, block_idx] += alpha_positive * (reward - Q[action, block_idx]) + curiosity_bonus
                block_views[block_idx] += 1
            elif i == 1: 
                block_idx = bad_block_idx; action = 1; reward = rewards[0]; 
                curiosity_bonus = curiosity / (1 + block_views[block_idx])
                Q[action, block_idx] += alpha_negative * (reward - Q[action, block_idx]) + curiosity_bonus
                block_views[block_idx] += 1
            else: 
                block_idx = good_block_idx[i - 1]
                softmax = np.array([0.5, 0.5])
                softmax[0] = 1 / np.sum(np.exp(beta * (Q[:, block_idx] - Q[0, block_idx])))
                softmax[1] = 1 - softmax[0]
                action = np.random.choice(range(2), p=softmax)
                if action == 0: # avoid
                    reward = 0 
                else: # approach
                    reward = rewards[int(block_idx != bad_block_idx)]
                    alpha = alpha_positive if reward > 0 else alpha_negative
                    curiosity_bonus = curiosity / (1 + block_views[block_idx])
                    Q[action, block_idx] += alpha * (reward - Q[action, block_idx]) + curiosity_bonus
                    block_views[block_idx] += 1
            Data = np.vstack((Data, 
                              np.array([participant, trial, action, reward, blocks[block_idx]], dtype = object)))
            trial += 1
        # subsequent trial set 
        for trialset in range(2, trialSets + 1): 
            for block_idx in np.random.permutation(4): 
                # determine approach vs. avoid - bernoulli trial
                softmax = np.array([0.5, 0.5])
                softmax[0] = 1 / np.sum(np.exp(beta * (Q[:, block_idx] - Q[0, block_idx])))
                softmax[1] = 1 - softmax[0]
                action = np.random.choice(range(2), p=softmax)
                # update values depending on action
                if action == 0: # avoid
                    reward = 0 
                else: # approach
                    reward = rewards[int(block_idx != bad_block_idx)]
                    alpha = alpha_positive if reward > 0 else alpha_negative
                    # compute the curiosity bonus with the number of encounter with the block and decay rate as input
                    curiosity_bonus = curiosity / (1 + block_views[block_idx])
                    Q[action, block_idx] += alpha * (reward - Q[action, block_idx]) + curiosity_bonus
                    block_views[block_idx] += 1
                # store the data
                Data = np.vstack((Data, 
                                  np.array([participant, trial, action, reward, blocks[block_idx]], dtype = object)))
                trial += 1
                
    return Data[1:]  # remove first row

def RLu(param, numParticipants, trialSets): 
    """
    Generates simulated data with the two-factor Q-learning model, i.e. the Q function 
    takes as input both the block currently presented to the participant and the action 
    available to them overtime. This is an extension/modification of the RL function. 
    """
    # parameter extraction 
    beta = param[0]
    alpha = param[1]
    utility_decay = param[2]

    blocks = np.array(["BP", "BS", "WP", "WS"])
    rewards = np.array([-2.0, 1.0])
    Data = np.ones(5)
        
    for participant in range(1, numParticipants + 1): 
        total_reward = 0 # keep track of total reward
        trial = 1 # counter for the trial number
        bad_block_idx = np.random.choice(4) # select a bad block for the current participant
        Q = np.full((2, 4), 0.5) # set up Q-function
        Q[0, :] = 0.0
        Q[1, :] = -0.5
        # first trial set - the first two blocks are shown and the second must be non-zaff
        good_block_idx = np.delete(np.random.permutation(4), bad_block_idx)
        for i in range(4): 
            if i == 0: 
                block_idx = good_block_idx[i]; action = 1; reward = rewards[1];
                scaled_reward = (total_reward + reward) ** utility_decay
                Q[action, block_idx] += alpha * (scaled_reward - Q[action, block_idx])
                total_reward += reward
            elif i == 1: 
                block_idx = bad_block_idx; action = 1; reward = rewards[0]; 
                scaled_reward = (total_reward + reward) ** utility_decay
                Q[action, block_idx] += alpha * (scaled_reward - Q[action, block_idx])
                total_reward += reward
            else: 
                block_idx = good_block_idx[i - 1]
                softmax = np.array([0.5, 0.5])
                softmax[0] = 1 / np.sum(np.exp(beta * (Q[:, block_idx] - Q[0, block_idx])))
                softmax[1] = 1 - softmax[0]
                action = np.random.choice(range(2), p=softmax)
                if action == 0: # avoid
                    reward = 0 
                else: # approach
                    reward = rewards[int(block_idx != bad_block_idx)]
                    scaled_reward = (total_reward + reward) ** utility_decay
                    Q[action, block_idx] += alpha * (scaled_reward - Q[action, block_idx])
                    total_reward += reward
            Data = np.vstack((Data, np.array([participant, trial, action, reward, blocks[block_idx]], dtype = object)))
            trial += 1
        # subsequent trial set 
        for trialset in range(2, trialSets + 1): 
            for block_idx in np.random.permutation(4): 
                # determine approach vs. avoid - bernoulli trial
                softmax = np.array([0.5, 0.5])
                softmax[0] = 1 / np.sum(np.exp(beta * (Q[:, block_idx] - Q[0, block_idx])))
                softmax[1] = 1 - softmax[0]
                action = np.random.choice(range(2), p=softmax)
                # update values depending on action
                if action == 0: # avoid
                    reward = 0 
                else: # approach
                    reward = rewards[int(block_idx != bad_block_idx)]
                    scaled_reward = (total_reward + reward) ** utility_decay
                    Q[action, block_idx] += alpha * (scaled_reward - Q[action, block_idx])
                    total_reward += reward
                # store the data
                Data = np.vstack((Data, np.array([participant, trial, action, reward, blocks[block_idx]], dtype = object)))
                trial += 1
                
    return Data[1:]  # remove first row

def RL_2D(param, numParticipants, trialSets): 
    """
    Generates simulated data using a reinforment learning model with two-dimensional rules, i.e. 
    a value (Q) function for both color and pattern. 
    """
    # set-up
    beta = param[0]
    alpha = param[1]
    blocks = np.array(["BP", "BS", "WP", "WS"])
    rewards = np.array([-2.0, 1.0])
    Data = np.ones(5)
        
    for participant in range(1, numParticipants + 1): 
        trial = 1 # counter for the trial number
        bad_block_idx = np.random.choice(4) # select a bad block for the current participant
        # Set up the Q-function for the patterns 
        Q_pattern = {}
        Q_pattern[0] = {"P": 0.0, "S": 0.0}
        Q_pattern[1] = {"P": -0.5, "S": -0.5}
        # Set up the Q-function for the colors 
        Q_color = {}
        Q_color[0] = {"B": 0.0, "W": 0.0}
        Q_color[1] = {"B": -0.5, "W": -0.5}
        # first trial set - the first two blocks are shown and the second must be non-zaff
        good_block_idx = np.delete(np.random.permutation(4), bad_block_idx)
        for i in range(4): 
            if i == 0: 
                block_idx = good_block_idx[i]; action = 1; reward = rewards[1]
                block = blocks[block_idx]; color = block[0]; pattern = block[1]
                # update Q-values 
                Q_pattern[action][pattern] += alpha * (reward - Q_pattern[action][pattern])
                Q_color[action][color] += alpha * (reward - Q_color[action][color])
            elif i == 1: 
                block_idx = bad_block_idx; action = 1; reward = rewards[0]; 
                block = blocks[block_idx]; color = block[0]; pattern = block[1]
                # update Q-values 
                Q_pattern[action][pattern] += alpha * (reward - Q_pattern[action][pattern])
                Q_color[action][color] += alpha * (reward - Q_color[action][color])
            else: 
                block_idx = good_block_idx[i - 1]
                block = blocks[block_idx]; color = block[0]; pattern = block[1]
                # compute the joint policy specific for this block from the individual policies 
                Q = np.full(2, np.nan)
                Q[0] = Q_color[0][color] * Q_pattern[0][pattern]
                Q[1] = Q_color[1][color] * Q_pattern[1][pattern]
                # compute the probability for taking either action
                softmax = np.full(2, np.nan)
                softmax[0] = 1 / np.sum(np.exp(beta * (Q - Q[0])))
                softmax[1] = 1 - softmax[0]
                action = np.random.choice(range(2), p=softmax)
                if action == 0: # avoid
                    reward = 0 
                else: # approach
                    reward = rewards[int(block_idx != bad_block_idx)]
                    Q_color[action][color] += alpha * (reward - Q_color[action][color])
                    Q_pattern[action][pattern] += alpha * (reward - Q_pattern[action][pattern])
            Data = np.vstack((Data, np.array([participant, trial, action, reward, blocks[block_idx]], dtype = object)))
            trial += 1
        # subsequent trial set 
        for trialset in range(2, trialSets + 1): 
            for block_idx in np.random.permutation(4): 
                # extract color and pattern from block identity
                block = blocks[block_idx]
                color = block[0]
                pattern = block[1]
                # compute the joint policy
                Q = np.full(2, np.nan)
                Q[0] = Q_color[0][color] * Q_pattern[0][pattern]
                Q[1] = Q_color[1][color] * Q_pattern[1][pattern]
                # compute the softmax probability for each action
                softmax = np.full(2, np.nan)
                softmax[0] = 1 / np.sum(np.exp(beta * (Q - Q[0])))
                softmax[1] = 1 - softmax[0]
                # determine approach vs. avoid - bernoulli trial
                action = np.random.choice(range(2), p=softmax)
                # update values depending on action
                if action == 0: # avoid
                    reward = 0 
                else: # approach
                    reward = rewards[int(block_idx != bad_block_idx)]
                    Q_color[action][color] += alpha * (reward - Q_color[action][color])
                    Q_pattern[action][pattern] += alpha * (reward - Q_pattern[action][pattern])
                # store the data
                Data = np.vstack((Data, np.array([participant, trial, action, reward, blocks[block_idx]], dtype = object)))
                trial += 1
                
    return Data[1:]  # remove first row

def RL_2D2a(param, numParticipants, trialSets): 
    """
    Generates simulated data using a reinforment learning model with two-dimensional rules, i.e. 
    a value (Q) function for both color and pattern. 
    """
    # set-up
    beta = param[0]
    alpha_color = param[1]
    alpha_pattern = param[2]
    blocks = np.array(["BP", "BS", "WP", "WS"])
    rewards = np.array([-2.0, 1.0])
    Data = np.ones(5)
        
    for participant in range(1, numParticipants + 1): 
        trial = 1 # counter for the trial number
        bad_block_idx = np.random.choice(4) # select a bad block for the current participant
        # Set up the Q-function for the patterns 
        Q_pattern = {}
        Q_pattern[0] = {"P": 0.0, "S": 0.0}
        Q_pattern[1] = {"P": -0.5, "S": -0.5}
        # Set up the Q-function for the colors 
        Q_color = {}
        Q_color[0] = {"B": 0.0, "W": 0.0}
        Q_color[1] = {"B": -0.5, "W": -0.5}
        # first trial set - the first two blocks are shown and the second must be non-zaff
        good_block_idx = np.delete(np.random.permutation(4), bad_block_idx)
        for i in range(4): 
            if i == 0: 
                block_idx = good_block_idx[i]; action = 1; reward = rewards[1]
                block = blocks[block_idx]; color = block[0]; pattern = block[1]
                # update Q-values 
                Q_pattern[action][pattern] += alpha_pattern * (reward - Q_pattern[action][pattern])
                Q_color[action][color] += alpha_color * (reward - Q_color[action][color])
            elif i == 1: 
                block_idx = bad_block_idx; action = 1; reward = rewards[0]; 
                block = blocks[block_idx]; color = block[0]; pattern = block[1]
                # update Q-values 
                Q_pattern[action][pattern] += alpha_pattern * (reward - Q_pattern[action][pattern])
                Q_color[action][color] += alpha_color * (reward - Q_color[action][color])
            else: 
                block_idx = good_block_idx[i - 1]
                block = blocks[block_idx]; color = block[0]; pattern = block[1]
                # compute the joint policy specific for this block from the individual policies 
                Q = np.full(2, np.nan)
                Q[0] = Q_color[0][color] * Q_pattern[0][pattern]
                Q[1] = Q_color[1][color] * Q_pattern[1][pattern]
                # compute the probability for taking either action
                softmax = np.full(2, np.nan)
                softmax[0] = 1 / np.sum(np.exp(beta * (Q - Q[0])))
                softmax[1] = 1 - softmax[0]
                action = np.random.choice(range(2), p=softmax)
                if action == 0: # avoid
                    reward = 0 
                else: # approach
                    reward = rewards[int(block_idx != bad_block_idx)]
                    Q_color[action][color] += alpha_color * (reward - Q_color[action][color])
                    Q_pattern[action][pattern] += alpha_pattern * (reward - Q_pattern[action][pattern])
            Data = np.vstack((Data, np.array([participant, trial, action, reward, blocks[block_idx]], dtype = object)))
            trial += 1
        # subsequent trial set 
        for trialset in range(2, trialSets + 1): 
            for block_idx in np.random.permutation(4): 
                # extract color and pattern from block identity
                block = blocks[block_idx]
                color = block[0]
                pattern = block[1]
                # compute the joint policy
                Q = np.full(2, np.nan)
                Q[0] = Q_color[0][color] * Q_pattern[0][pattern]
                Q[1] = Q_color[1][color] * Q_pattern[1][pattern]
                # compute the softmax probability for each action
                softmax = np.full(2, np.nan)
                softmax[0] = 1 / np.sum(np.exp(beta * (Q - Q[0])))
                softmax[1] = 1 - softmax[0]
                # determine approach vs. avoid - bernoulli trial
                action = np.random.choice(range(2), p=softmax)
                # update values depending on action
                if action == 0: # avoid
                    reward = 0 
                else: # approach
                    reward = rewards[int(block_idx != bad_block_idx)]
                    Q_color[action][color] += alpha_color * (reward - Q_color[action][color])
                    Q_pattern[action][pattern] += alpha_pattern * (reward - Q_pattern[action][pattern])
                # store the data
                Data = np.vstack((Data, np.array([participant, trial, action, reward, blocks[block_idx]], dtype = object)))
                trial += 1
                
    return Data[1:]  # remove first row

def RL2a_2D(param, numParticipants, trialSets): 
    """
    Generates simulated data using a reinforment learning model with two-dimensional rules, i.e. 
    a value (Q) function for both color and pattern. 
    """
    # set-up
    beta = param[0]
    alpha_positive = param[1]
    alpha_negative = param[2]
    blocks = np.array(["BP", "BS", "WP", "WS"])
    rewards = np.array([-2.0, 1.0])
    Data = np.ones(5)
        
    for participant in range(1, numParticipants + 1): 
        trial = 1 # counter for the trial number
        bad_block_idx = np.random.choice(4) # select a bad block for the current participant
        # Set up the Q-function for the patterns 
        Q_pattern = {}
        Q_pattern[0] = {"P": 0.0, "S": 0.0}
        Q_pattern[1] = {"P": -0.5, "S": -0.5}
        # Set up the Q-function for the colors 
        Q_color = {}
        Q_color[0] = {"B": 0.0, "W": 0.0}
        Q_color[1] = {"B": -0.5, "W": -0.5}
        # first trial set - the first two blocks are shown and the second must be non-zaff
        good_block_idx = np.delete(np.random.permutation(4), bad_block_idx)
        for i in range(4): 
            if i == 0: 
                block_idx = good_block_idx[i]; action = 1; reward = rewards[1]
                block = blocks[block_idx]; color = block[0]; pattern = block[1]
                # update Q-values 
                Q_pattern[action][pattern] += alpha_positive * (reward - Q_pattern[action][pattern])
                Q_color[action][color] += alpha_positive * (reward - Q_color[action][color])
            elif i == 1: 
                block_idx = bad_block_idx; action = 1; reward = rewards[0]; 
                block = blocks[block_idx]; color = block[0]; pattern = block[1]
                # update Q-values 
                Q_pattern[action][pattern] += alpha_negative * (reward - Q_pattern[action][pattern])
                Q_color[action][color] += alpha_negative * (reward - Q_color[action][color])
            else: 
                block_idx = good_block_idx[i - 1]
                block = blocks[block_idx]; color = block[0]; pattern = block[1]
                # compute the joint policy specific for this block from the individual policies 
                Q = np.full(2, np.nan)
                Q[0] = Q_color[0][color] * Q_pattern[0][pattern]
                Q[1] = Q_color[1][color] * Q_pattern[1][pattern]
                # compute the probability for taking either action
                softmax = np.full(2, np.nan)
                softmax[0] = 1 / np.sum(np.exp(beta * (Q - Q[0])))
                softmax[1] = 1 - softmax[0]
                action = np.random.choice(range(2), p=softmax)
                if action == 0: # avoid
                    reward = 0 
                else: # approach
                    reward = rewards[int(block_idx != bad_block_idx)]
                    if reward > 0: 
                        alpha = alpha_positive
                    else: 
                        alpha = alpha_negative
                    Q_color[action][color] += alpha * (reward - Q_color[action][color])
                    Q_pattern[action][pattern] += alpha * (reward - Q_pattern[action][pattern])
            Data = np.vstack((Data, np.array([participant, trial, action, reward, blocks[block_idx]], dtype = object)))
            trial += 1
        # subsequent trial set 
        for trialset in range(2, trialSets + 1): 
            for block_idx in np.random.permutation(4): 
                # extract color and pattern from block identity
                block = blocks[block_idx]
                color = block[0]
                pattern = block[1]
                # compute the joint policy
                Q = np.full(2, np.nan)
                Q[0] = Q_color[0][color] * Q_pattern[0][pattern]
                Q[1] = Q_color[1][color] * Q_pattern[1][pattern]
                # compute the softmax probability for each action
                softmax = np.full(2, np.nan)
                softmax[0] = 1 / np.sum(np.exp(beta * (Q - Q[0])))
                softmax[1] = 1 - softmax[0]
                # determine approach vs. avoid - bernoulli trial
                action = np.random.choice(range(2), p=softmax)
                # update values depending on action
                if action == 0: # avoid
                    reward = 0 
                else: # approach
                    reward = rewards[int(block_idx != bad_block_idx)]
                    if reward > 0: 
                        alpha = alpha_positive
                    else: 
                        alpha = alpha_negative
                    Q_color[action][color] += alpha * (reward - Q_color[action][color])
                    Q_pattern[action][pattern] += alpha * (reward - Q_pattern[action][pattern])
                # store the data
                Data = np.vstack((Data, np.array([participant, trial, action, reward, blocks[block_idx]], dtype = object)))
                trial += 1
                
    return Data[1:]  # remove first row

def RL2a_2D2a(param, numParticipants, trialSets): 
    """
    Generates simulated data using a reinforment learning model with two-dimensional rules, i.e. 
    a value (Q) function for both color and pattern. 
    """
    # set-up
    beta = param[0]
    alpha_color_positive = param[1]
    alpha_color_negative = param[2]
    alpha_pattern_positive = param[3]
    alpha_pattern_negative = param[4]
    blocks = np.array(["BP", "BS", "WP", "WS"])
    rewards = np.array([-2.0, 1.0])
    Data = np.ones(5)
        
    for participant in range(1, numParticipants + 1): 
        trial = 1 # counter for the trial number
        bad_block_idx = np.random.choice(4) # select a bad block for the current participant
        # Set up the Q-function for the patterns 
        Q_pattern = {}
        Q_pattern[0] = {"P": 0.0, "S": 0.0}
        Q_pattern[1] = {"P": -0.5, "S": -0.5}
        # Set up the Q-function for the colors 
        Q_color = {}
        Q_color[0] = {"B": 0.0, "W": 0.0}
        Q_color[1] = {"B": -0.5, "W": -0.5}
        # first trial set - the first two blocks are shown and the second must be non-zaff
        good_block_idx = np.delete(np.random.permutation(4), bad_block_idx)
        for i in range(4): 
            if i == 0: 
                block_idx = good_block_idx[i]; action = 1; reward = rewards[1]
                block = blocks[block_idx]; color = block[0]; pattern = block[1]
                # update Q-values 
                Q_pattern[action][pattern] += alpha_pattern_positive * (reward - Q_pattern[action][pattern])
                Q_color[action][color] += alpha_color_positive * (reward - Q_color[action][color])
            elif i == 1: 
                block_idx = bad_block_idx; action = 1; reward = rewards[0]; 
                block = blocks[block_idx]; color = block[0]; pattern = block[1]
                # update Q-values 
                Q_pattern[action][pattern] += alpha_pattern_negative * (reward - Q_pattern[action][pattern])
                Q_color[action][color] += alpha_color_negative * (reward - Q_color[action][color])
            else: 
                block_idx = good_block_idx[i - 1]
                block = blocks[block_idx]; color = block[0]; pattern = block[1]
                # compute the joint policy specific for this block from the individual policies 
                Q = np.full(2, np.nan)
                Q[0] = Q_color[0][color] * Q_pattern[0][pattern]
                Q[1] = Q_color[1][color] * Q_pattern[1][pattern]
                # compute the probability for taking either action
                softmax = np.full(2, np.nan)
                softmax[0] = 1 / np.sum(np.exp(beta * (Q - Q[0])))
                softmax[1] = 1 - softmax[0]
                action = np.random.choice(range(2), p=softmax)
                if action == 0: # avoid
                    reward = 0 
                else: # approach
                    reward = rewards[int(block_idx != bad_block_idx)]
                    alpha_color = alpha_color_positive if reward > 0 else alpha_color_negative
                    alpha_pattern = alpha_pattern_positive if reward > 0 else alpha_pattern_negative
                    Q_color[action][color] += alpha_color * (reward - Q_color[action][color])
                    Q_pattern[action][pattern] += alpha_pattern * (reward - Q_pattern[action][pattern])
            Data = np.vstack((Data, np.array([participant, trial, action, reward, blocks[block_idx]], dtype = object)))
            trial += 1
        # subsequent trial set 
        for trialset in range(2, trialSets + 1): 
            for block_idx in np.random.permutation(4): 
                # extract color and pattern from block identity
                block = blocks[block_idx]
                color = block[0]
                pattern = block[1]
                # compute the joint policy
                Q = np.full(2, np.nan)
                Q[0] = Q_color[0][color] * Q_pattern[0][pattern]
                Q[1] = Q_color[1][color] * Q_pattern[1][pattern]
                # compute the softmax probability for each action
                softmax = np.full(2, np.nan)
                softmax[0] = 1 / np.sum(np.exp(beta * (Q - Q[0])))
                softmax[1] = 1 - softmax[0]
                # determine approach vs. avoid - bernoulli trial
                action = np.random.choice(range(2), p=softmax)
                # update values depending on action
                if action == 0: # avoid
                    reward = 0 
                else: # approach
                    reward = rewards[int(block_idx != bad_block_idx)]
                    alpha_color = alpha_color_positive if reward > 0 else alpha_color_negative
                    alpha_pattern = alpha_pattern_positive if reward > 0 else alpha_pattern_negative
                    Q_color[action][color] += alpha_color * (reward - Q_color[action][color])
                    Q_pattern[action][pattern] += alpha_pattern * (reward - Q_pattern[action][pattern])
                # store the data
                Data = np.vstack((Data, np.array([participant, trial, action, reward, blocks[block_idx]], dtype = object)))
                trial += 1
                
    return Data[1:]  # remove first row

def RLc_2D(param, numParticipants, trialSets): 
    """
    Generates simulated data using a reinforment learning model with two-dimensional rules, i.e. 
    a value (Q) function for both color and pattern. 
    """
    # set-up
    beta = param[0]
    alpha = param[1]
    curiosity = param[2]
    blocks = np.array(["BP", "BS", "WP", "WS"])
    rewards = np.array([-2.0, 1.0])
    Data = np.ones(5)
        
    for participant in range(1, numParticipants + 1): 
        block_views = np.full(4, 0)
        trial = 1 # counter for the trial number
        bad_block_idx = np.random.choice(4) # select a bad block for the current participant
        # Set up the Q-function for the patterns 
        Q_pattern = {}
        Q_pattern[0] = {"P": 0.0, "S": 0.0}
        Q_pattern[1] = {"P": -0.5, "S": -0.5}
        # Set up the Q-function for the colors 
        Q_color = {}
        Q_color[0] = {"B": 0.0, "W": 0.0}
        Q_color[1] = {"B": -0.5, "W": -0.5}
        # first trial set - the first two blocks are shown and the second must be non-zaff
        good_block_idx = np.delete(np.random.permutation(4), bad_block_idx)
        for i in range(4): 
            if i == 0: 
                block_idx = good_block_idx[i]; action = 1; reward = rewards[1]
                block = blocks[block_idx]; color = block[0]; pattern = block[1]
                curiosity_bonus = curiosity / (1 + block_views[block_idx])
                block_views[block_idx] += 1
                # update Q-values 
                Q_pattern[action][pattern] += alpha * (reward - Q_pattern[action][pattern]) + curiosity_bonus
                Q_color[action][color] += alpha * (reward - Q_color[action][color]) + curiosity_bonus
            elif i == 1: 
                block_idx = bad_block_idx; action = 1; reward = rewards[0]; 
                block = blocks[block_idx]; color = block[0]; pattern = block[1]
                curiosity_bonus = curiosity / (1 + block_views[block_idx])
                block_views[block_idx] += 1
                # update Q-values 
                Q_pattern[action][pattern] += alpha * (reward - Q_pattern[action][pattern]) + curiosity_bonus
                Q_color[action][color] += alpha * (reward - Q_color[action][color]) + curiosity_bonus
            else: 
                block_idx = good_block_idx[i - 1]
                block = blocks[block_idx]; color = block[0]; pattern = block[1]
                # compute the joint policy specific for this block from the individual policies 
                Q = np.full(2, np.nan)
                Q[0] = Q_color[0][color] * Q_pattern[0][pattern]
                Q[1] = Q_color[1][color] * Q_pattern[1][pattern]
                # compute the probability for taking either action
                softmax = np.full(2, np.nan)
                softmax[0] = 1 / np.sum(np.exp(beta * (Q - Q[0])))
                softmax[1] = 1 - softmax[0]
                action = np.random.choice(range(2), p=softmax)
                if action == 0: # avoid
                    reward = 0 
                else: # approach
                    reward = rewards[int(block_idx != bad_block_idx)]
                    curiosity_bonus = curiosity / (1 + block_views[block_idx])
                    block_views[block_idx] += 1
                    Q_color[action][color] += alpha * (reward - Q_color[action][color]) + curiosity_bonus
                    Q_pattern[action][pattern] += alpha * (reward - Q_pattern[action][pattern]) + curiosity_bonus
            Data = np.vstack((Data, np.array([participant, trial, action, reward, blocks[block_idx]], dtype = object)))
            trial += 1
        # subsequent trial set 
        for trialset in range(2, trialSets + 1): 
            for block_idx in np.random.permutation(4): 
                # extract color and pattern from block identity
                block = blocks[block_idx]
                color = block[0]
                pattern = block[1]
                # compute the joint policy
                Q = np.full(2, np.nan)
                Q[0] = Q_color[0][color] * Q_pattern[0][pattern]
                Q[1] = Q_color[1][color] * Q_pattern[1][pattern]
                # compute the softmax probability for each action
                softmax = np.full(2, np.nan)
                softmax[0] = 1 / np.sum(np.exp(beta * (Q - Q[0])))
                softmax[1] = 1 - softmax[0]
                # determine approach vs. avoid - bernoulli trial
                action = np.random.choice(range(2), p=softmax)
                # update values depending on action
                if action == 0: # avoid
                    reward = 0 
                else: # approach
                    reward = rewards[int(block_idx != bad_block_idx)]
                    curiosity_bonus = curiosity / (1 + block_views[block_idx])
                    block_views[block_idx] += 1
                    Q_color[action][color] += alpha * (reward - Q_color[action][color]) + curiosity_bonus
                    Q_pattern[action][pattern] += alpha * (reward - Q_pattern[action][pattern]) + curiosity_bonus
                # store the data
                Data = np.vstack((Data, np.array([participant, trial, action, reward, blocks[block_idx]], dtype = object)))
                trial += 1
                
    return Data[1:]  # remove first row

def RL2ac_2D(param, numParticipants, trialSets): 
    """
    Generates simulated data using a reinforment learning model with two-dimensional rules, i.e. 
    a value (Q) function for both color and pattern. 
    """
    # set-up
    beta = param[0]
    alpha_positive = param[1]
    alpha_negative = param[2]
    curiosity = param[3]
    blocks = np.array(["BP", "BS", "WP", "WS"])
    rewards = np.array([-2.0, 1.0])
    Data = np.ones(5)
        
    for participant in range(1, numParticipants + 1): 
        block_views = np.full(4, 0)
        trial = 1 # counter for the trial number
        bad_block_idx = np.random.choice(4) # select a bad block for the current participant
        # Set up the Q-function for the patterns 
        Q_pattern = {}
        Q_pattern[0] = {"P": 0.0, "S": 0.0}
        Q_pattern[1] = {"P": -0.5, "S": -0.5}
        # Set up the Q-function for the colors 
        Q_color = {}
        Q_color[0] = {"B": 0.0, "W": 0.0}
        Q_color[1] = {"B": -0.5, "W": -0.5}
        # first trial set - the first two blocks are shown and the second must be non-zaff
        good_block_idx = np.delete(np.random.permutation(4), bad_block_idx)
        for i in range(4): 
            if i == 0: 
                block_idx = good_block_idx[i]; action = 1; reward = rewards[1]
                block = blocks[block_idx]; color = block[0]; pattern = block[1]
                curiosity_bonus = curiosity / (1 + block_views[block_idx])
                block_views[block_idx] += 1
                # update Q-values 
                Q_pattern[action][pattern] += alpha_positive * (reward - Q_pattern[action][pattern]) + curiosity_bonus
                Q_color[action][color] += alpha_positive * (reward - Q_color[action][color]) + curiosity_bonus
            elif i == 1: 
                block_idx = bad_block_idx; action = 1; reward = rewards[0]; 
                block = blocks[block_idx]; color = block[0]; pattern = block[1]
                curiosity_bonus = curiosity / (1 + block_views[block_idx])
                block_views[block_idx] += 1
                # update Q-values 
                Q_pattern[action][pattern] += alpha_negative * (reward - Q_pattern[action][pattern]) + curiosity_bonus
                Q_color[action][color] += alpha_negative * (reward - Q_color[action][color]) + curiosity_bonus
            else: 
                block_idx = good_block_idx[i - 1]
                block = blocks[block_idx]; color = block[0]; pattern = block[1]
                # compute the joint policy specific for this block from the individual policies 
                Q = np.full(2, np.nan)
                Q[0] = Q_color[0][color] * Q_pattern[0][pattern]
                Q[1] = Q_color[1][color] * Q_pattern[1][pattern]
                # compute the probability for taking either action
                softmax = np.full(2, np.nan)
                softmax[0] = 1 / np.sum(np.exp(beta * (Q - Q[0])))
                softmax[1] = 1 - softmax[0]
                action = np.random.choice(range(2), p=softmax)
                if action == 0: # avoid
                    reward = 0 
                else: # approach
                    reward = rewards[int(block_idx != bad_block_idx)]
                    curiosity_bonus = curiosity / (1 + block_views[block_idx])
                    block_views[block_idx] += 1
                    alpha = alpha_positive if reward > 0 else alpha_negative
                    Q_color[action][color] += alpha * (reward - Q_color[action][color]) + curiosity_bonus
                    Q_pattern[action][pattern] += alpha * (reward - Q_pattern[action][pattern]) + curiosity_bonus
            Data = np.vstack((Data, np.array([participant, trial, action, reward, blocks[block_idx]], dtype = object)))
            trial += 1
        # subsequent trial set 
        for trialset in range(2, trialSets + 1): 
            for block_idx in np.random.permutation(4): 
                # extract color and pattern from block identity
                block = blocks[block_idx]
                color = block[0]
                pattern = block[1]
                # compute the joint policy
                Q = np.full(2, np.nan)
                Q[0] = Q_color[0][color] * Q_pattern[0][pattern]
                Q[1] = Q_color[1][color] * Q_pattern[1][pattern]
                # compute the softmax probability for each action
                softmax = np.full(2, np.nan)
                softmax[0] = 1 / np.sum(np.exp(beta * (Q - Q[0])))
                softmax[1] = 1 - softmax[0]
                # determine approach vs. avoid - bernoulli trial
                action = np.random.choice(range(2), p=softmax)
                # update values depending on action
                if action == 0: # avoid
                    reward = 0 
                else: # approach
                    reward = rewards[int(block_idx != bad_block_idx)]
                    curiosity_bonus = curiosity / (1 + block_views[block_idx])
                    block_views[block_idx] += 1
                    alpha = alpha_positive if reward > 0 else alpha_negative
                    Q_color[action][color] += alpha * (reward - Q_color[action][color]) + curiosity_bonus
                    Q_pattern[action][pattern] += alpha * (reward - Q_pattern[action][pattern]) + curiosity_bonus
                # store the data
                Data = np.vstack((Data, np.array([participant, trial, action, reward, blocks[block_idx]], dtype = object)))
                trial += 1
                
    return Data[1:]  # remove first row

def RLc_2D2a(param, numParticipants, trialSets): 
    """
    Generates simulated data using a reinforment learning model with two-dimensional rules, i.e. 
    a value (Q) function for both color and pattern. 
    """
    # set-up
    beta = param[0]
    alpha_color = param[1]
    alpha_pattern = param[2]
    curiosity = param[3]
    blocks = np.array(["BP", "BS", "WP", "WS"])
    rewards = np.array([-2.0, 1.0])
    Data = np.ones(5)
        
    for participant in range(1, numParticipants + 1): 
        block_views = np.full(4, 0)
        trial = 1 # counter for the trial number
        bad_block_idx = np.random.choice(4) # select a bad block for the current participant
        # Set up the Q-function for the patterns 
        Q_pattern = {}
        Q_pattern[0] = {"P": 0.0, "S": 0.0}
        Q_pattern[1] = {"P": -0.5, "S": -0.5}
        # Set up the Q-function for the colors 
        Q_color = {}
        Q_color[0] = {"B": 0.0, "W": 0.0}
        Q_color[1] = {"B": -0.5, "W": -0.5}
        # first trial set - the first two blocks are shown and the second must be non-zaff
        good_block_idx = np.delete(np.random.permutation(4), bad_block_idx)
        for i in range(4): 
            if i == 0: 
                block_idx = good_block_idx[i]; action = 1; reward = rewards[1]
                block = blocks[block_idx]; color = block[0]; pattern = block[1]
                curiosity_bonus = curiosity / (1 + block_views[block_idx])
                block_views[block_idx] += 1
                # update Q-values 
                Q_pattern[action][pattern] += alpha_pattern * (reward - Q_pattern[action][pattern]) + curiosity_bonus
                Q_color[action][color] += alpha_color * (reward - Q_color[action][color]) + curiosity_bonus
            elif i == 1: 
                block_idx = bad_block_idx; action = 1; reward = rewards[0]; 
                block = blocks[block_idx]; color = block[0]; pattern = block[1]
                curiosity_bonus = curiosity / (1 + block_views[block_idx])
                block_views[block_idx] += 1
                # update Q-values 
                Q_pattern[action][pattern] += alpha_pattern * (reward - Q_pattern[action][pattern]) + curiosity_bonus
                Q_color[action][color] += alpha_color * (reward - Q_color[action][color]) + curiosity_bonus
            else: 
                block_idx = good_block_idx[i - 1]
                block = blocks[block_idx]; color = block[0]; pattern = block[1]
                # compute the joint policy specific for this block from the individual policies 
                Q = np.full(2, np.nan)
                Q[0] = Q_color[0][color] * Q_pattern[0][pattern]
                Q[1] = Q_color[1][color] * Q_pattern[1][pattern]
                # compute the probability for taking either action
                softmax = np.full(2, np.nan)
                softmax[0] = 1 / np.sum(np.exp(beta * (Q - Q[0])))
                softmax[1] = 1 - softmax[0]
                action = np.random.choice(range(2), p=softmax)
                if action == 0: # avoid
                    reward = 0 
                else: # approach
                    reward = rewards[int(block_idx != bad_block_idx)]
                    curiosity_bonus = curiosity / (1 + block_views[block_idx])
                    block_views[block_idx] += 1
                    Q_color[action][color] += alpha_color * (reward - Q_color[action][color]) + curiosity_bonus
                    Q_pattern[action][pattern] += alpha_pattern * (reward - Q_pattern[action][pattern]) + curiosity_bonus
            Data = np.vstack((Data, np.array([participant, trial, action, reward, blocks[block_idx]], dtype = object)))
            trial += 1
        # subsequent trial set 
        for trialset in range(2, trialSets + 1): 
            for block_idx in np.random.permutation(4): 
                # extract color and pattern from block identity
                block = blocks[block_idx]
                color = block[0]
                pattern = block[1]
                # compute the joint policy
                Q = np.full(2, np.nan)
                Q[0] = Q_color[0][color] * Q_pattern[0][pattern]
                Q[1] = Q_color[1][color] * Q_pattern[1][pattern]
                # compute the softmax probability for each action
                softmax = np.full(2, np.nan)
                softmax[0] = 1 / np.sum(np.exp(beta * (Q - Q[0])))
                softmax[1] = 1 - softmax[0]
                # determine approach vs. avoid - bernoulli trial
                action = np.random.choice(range(2), p=softmax)
                # update values depending on action
                if action == 0: # avoid
                    reward = 0 
                else: # approach
                    reward = rewards[int(block_idx != bad_block_idx)]
                    curiosity_bonus = curiosity / (1 + block_views[block_idx])
                    block_views[block_idx] += 1
                    Q_color[action][color] += alpha_color * (reward - Q_color[action][color]) + curiosity_bonus
                    Q_pattern[action][pattern] += alpha_pattern * (reward - Q_pattern[action][pattern]) + curiosity_bonus
                # store the data
                Data = np.vstack((Data, np.array([participant, trial, action, reward, blocks[block_idx]], dtype = object)))
                trial += 1
                
    return Data[1:]  # remove first row

def RL2ac_2D2a(param, numParticipants, trialSets): 
    """
    Generates simulated data using a reinforment learning model with two-dimensional rules, i.e. 
    a value (Q) function for both color and pattern. 
    """
    # set-up
    beta = param[0]
    alpha_color_positive = param[1]
    alpha_color_negative = param[2]
    alpha_pattern_positive = param[3]
    alpha_pattern_negative = param[4]
    curiosity = param[5]
    blocks = np.array(["BP", "BS", "WP", "WS"])
    rewards = np.array([-2.0, 1.0])
    Data = np.ones(5)
        
    for participant in range(1, numParticipants + 1): 
        block_views = np.full(4, 0)
        trial = 1 # counter for the trial number
        bad_block_idx = np.random.choice(4) # select a bad block for the current participant
        # Set up the Q-function for the patterns 
        Q_pattern = {}
        Q_pattern[0] = {"P": 0.0, "S": 0.0}
        Q_pattern[1] = {"P": -0.5, "S": -0.5}
        # Set up the Q-function for the colors 
        Q_color = {}
        Q_color[0] = {"B": 0.0, "W": 0.0}
        Q_color[1] = {"B": -0.5, "W": -0.5}
        # first trial set - the first two blocks are shown and the second must be non-zaff
        good_block_idx = np.delete(np.random.permutation(4), bad_block_idx)
        for i in range(4): 
            if i == 0: 
                block_idx = good_block_idx[i]; action = 1; reward = rewards[1]
                block = blocks[block_idx]; color = block[0]; pattern = block[1]
                curiosity_bonus = curiosity / (1 + block_views[block_idx])
                block_views[block_idx] += 1
                # update Q-values 
                Q_pattern[action][pattern] += alpha_pattern_positive * (reward - Q_pattern[action][pattern]) + curiosity_bonus
                Q_color[action][color] += alpha_color_positive * (reward - Q_color[action][color]) + curiosity_bonus
            elif i == 1: 
                block_idx = bad_block_idx; action = 1; reward = rewards[0]; 
                block = blocks[block_idx]; color = block[0]; pattern = block[1]
                curiosity_bonus = curiosity / (1 + block_views[block_idx])
                block_views[block_idx] += 1
                # update Q-values 
                Q_pattern[action][pattern] += alpha_pattern_negative * (reward - Q_pattern[action][pattern]) + curiosity_bonus
                Q_color[action][color] += alpha_color_negative * (reward - Q_color[action][color]) + curiosity_bonus
            else: 
                block_idx = good_block_idx[i - 1]
                block = blocks[block_idx]; color = block[0]; pattern = block[1]
                # compute the joint policy specific for this block from the individual policies 
                Q = np.full(2, np.nan)
                Q[0] = Q_color[0][color] * Q_pattern[0][pattern]
                Q[1] = Q_color[1][color] * Q_pattern[1][pattern]
                # compute the probability for taking either action
                softmax = np.full(2, np.nan)
                softmax[0] = 1 / np.sum(np.exp(beta * (Q - Q[0])))
                softmax[1] = 1 - softmax[0]
                action = np.random.choice(range(2), p=softmax)
                if action == 0: # avoid
                    reward = 0 
                else: # approach
                    reward = rewards[int(block_idx != bad_block_idx)]
                    curiosity_bonus = curiosity / (1 + block_views[block_idx])
                    block_views[block_idx] += 1
                    alpha_color = alpha_color_positive if reward > 0 else alpha_color_negative
                    alpha_pattern = alpha_pattern_positive if reward > 0 else alpha_pattern_negative
                    Q_color[action][color] += alpha_color * (reward - Q_color[action][color]) + curiosity_bonus
                    Q_pattern[action][pattern] += alpha_pattern * (reward - Q_pattern[action][pattern]) + curiosity_bonus
            Data = np.vstack((Data, np.array([participant, trial, action, reward, blocks[block_idx]], dtype = object)))
            trial += 1
        # subsequent trial set 
        for trialset in range(2, trialSets + 1): 
            for block_idx in np.random.permutation(4): 
                # extract color and pattern from block identity
                block = blocks[block_idx]
                color = block[0]
                pattern = block[1]
                # compute the joint policy
                Q = np.full(2, np.nan)
                Q[0] = Q_color[0][color] * Q_pattern[0][pattern]
                Q[1] = Q_color[1][color] * Q_pattern[1][pattern]
                # compute the softmax probability for each action
                softmax = np.full(2, np.nan)
                softmax[0] = 1 / np.sum(np.exp(beta * (Q - Q[0])))
                softmax[1] = 1 - softmax[0]
                # determine approach vs. avoid - bernoulli trial
                action = np.random.choice(range(2), p=softmax)
                # update values depending on action
                if action == 0: # avoid
                    reward = 0 
                else: # approach
                    reward = rewards[int(block_idx != bad_block_idx)]
                    curiosity_bonus = curiosity / (1 + block_views[block_idx])
                    block_views[block_idx] += 1
                    alpha_color = alpha_color_positive if reward > 0 else alpha_color_negative
                    alpha_pattern = alpha_pattern_positive if reward > 0 else alpha_pattern_negative
                    Q_color[action][color] += alpha_color * (reward - Q_color[action][color]) + curiosity_bonus
                    Q_pattern[action][pattern] += alpha_pattern * (reward - Q_pattern[action][pattern]) + curiosity_bonus
                # store the data
                Data = np.vstack((Data, np.array([participant, trial, action, reward, blocks[block_idx]], dtype = object)))
                trial += 1
                
    return Data[1:]  # remove first row
