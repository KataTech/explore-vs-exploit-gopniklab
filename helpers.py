###### This file stores all helper functions for analysis ######
import numpy as np
import matplotlib.pyplot as plt 
from scipy.optimize import minimize

##### Optimization #####

def optimize(fname, 
             bounds, 
             Data, 
             niter,
             toplot=False, 
             message=False
            ):
            
    """
    bestparameters,bestllh = optimize(fname, bounds, Data, niter, toplot) runs 
    the minimize function niter times on the function fname, with constraints 
    bounds to find parameters that best fit the data Data. It returns the 
    best likelihood and best fit parameters over the niter iterations.

    ## fname is the python function to optimize. fname 
    should take as first argument a 1 by n vector of parameters. 
    Note the bounds are set up differently than they are in Matlab, 
    And should come as a list of [min,max] pairs. (ie. [[min,max],[min,max], ...])
     
    ## Data is the data set to be fit by likelihood function fname.
    ## niter is the number of starting points for the optimization
    ## toplot is an optional argument; if plot~=0, this function will plot the
    best likelihood as a function of starting point iterations.
    
    ## best parameters is the 1*n vector of parameters found to minimize the
    negative log likelihood over the data.
    bestllh is the log likelihood value for the best parameters.
     
    """
    
    outcomes = np.full([niter, len(bounds)+1], np.nan)
    optimcurve = np.full(niter, np.nan)
    for i in range(niter):
        
        if message: 
            if (int(niter * 0.10) == i): 
                print("Processed 10% ....")
            elif (int(niter * 0.20) == i): 
                print("Processed 20% ....")
            elif (int(niter * 0.30) == i): 
                print("Processed 30% ....")
            elif (int(niter * 0.40) == i): 
                print("Processed 40% ....")
            elif (int(niter * 0.50) == i): 
                print("Processed 50% ....")
            elif (int(niter * 0.60) == i): 
                print("Processed 60% ....")
            elif (int(niter * 0.70) == i): 
                print("Processed 70% ....")
            elif (int(niter * 0.80) == i): 
                print("Processed 80% ....")
            elif (int(niter * 0.90) == i): 
                print("Processed 90% ....")
        
        # random starting point based on maximum bounds
        params0 = np.array([(bound[1] - bound[0]) * np.random.rand() + bound[0] for bound in bounds])
            
        # compute the function value at the starting point
        llh0 = fname(params0, Data)
        
        # run the optimizer with constraints
        result = minimize(fun=fname, x0=params0, args=(Data), bounds=bounds)
        x = result.x
        bestllh = fname(x, Data)
        outcomes[i, :] = [bestllh] + [xi for xi in x]    
        optimcurve[i] = min(outcomes[:(i+1), 0])

    if message: 
        print("Completed iterations... now compiling results...")
    
    # find the global minimum out of all outcomes
    i = np.argwhere(outcomes[:, 0] == np.min(outcomes[:, 0]))
    bestparameters = outcomes[i[0], 1:].flatten()
    bestllh = -1 * outcomes[i[0], 0].flatten()[0]
    
    # plot the best llh found by the optimizer as a function of iteration number.
    if toplot:
        plt.figure()
        plt.plot(range(niter), np.round(optimcurve, 6), 'o-')
        plt.yscale("log")
        plt.xlabel('iteration')
        plt.ylabel('best minimum')
        plt.title('Negative log likelihood')
    
    return(bestparameters, bestllh)

##### Visualization #####

def plot_llh_by_params(fname, bounds, Data, plot = 'contour', color_map = 'viridis', save=False, save_path=None): 
    """
    Create a 3D-plot of log-likelihood distribution based on the specified 
    bounds. 
    """
    beta_vector = np.linspace(bounds[0][0], bounds[0][1], 30)
    alpha_vector = np.linspace(bounds[1][0], bounds[1][1], 30)
    llh = np.full([30, 30], np.nan)
    for i in range(30): 
        for j in range(30): 
            llh[i][j] = fname(np.array([beta_vector[i], alpha_vector[j]]), Data)
    Beta, Alpha = np.meshgrid(beta_vector, alpha_vector)
    fig = plt.figure()
    ax = plt.axes(projection='3d')
    if plot == "contour": 
        ax.contour3D(Beta, Alpha, llh, 50, cmap=color_map)
    elif plot == "surface": 
        ax.plot_surface(Beta, Alpha, llh, cmap = color_map)
    ax.set_xlabel('beta')
    ax.set_ylabel('alpha')
    ax.set_zlabel('negative log likelihood')
    if save: 
        plt.savefig(save_path , dpi = 300)

##### Parameter Recovery #####

def parameter_recovery(bounds, gen_func, llh_func, seed = 555, nparticipants = 40, nsamples = 100, msg = False): 
    """
    Perform parameter recovery analysis for a generative model and its corresponding maximumlikelihood 
    functions given a set of boundaries. 
    """
    np.random.seed(seed)
    dimensions = bounds.shape[0]
    results = np.full((nsamples, 2, dimensions), np.nan)
    for sample in range(nsamples): 
        if msg: 
            niter = nsamples
            i = sample
            if (int(niter * 0.10) == i): 
                print("Processed 10% ....")
            elif (int(niter * 0.20) == i): 
                print("Processed 20% ....")
            elif (int(niter * 0.30) == i): 
                print("Processed 30% ....")
            elif (int(niter * 0.40) == i): 
                print("Processed 40% ....")
            elif (int(niter * 0.50) == i): 
                print("Processed 50% ....")
            elif (int(niter * 0.60) == i): 
                print("Processed 60% ....")
            elif (int(niter * 0.70) == i): 
                print("Processed 70% ....")
            elif (int(niter * 0.80) == i): 
                print("Processed 80% ....")
            elif (int(niter * 0.90) == i): 
                print("Processed 90% ....")
        params = np.array([(bound[1] - bound[0]) * np.random.rand() + bound[0] for bound in bounds])
        data = gen_func(params, nparticipants, 4)
        bestparameters, _ = optimize(llh_func, bounds, data, niter = 5, toplot = False, message = False)
        results[sample, 0, :] = params
        results[sample, 1, :] = bestparameters
    return results

def plot_param_recovery_analysis(x, y, title, xlabel=None, ylabel=None, 
                                 xmin = 0, xmax = 1, save = False, save_path = None): 
    fig, ax = plt.subplots()
    ax.scatter(x, y, s=60, alpha=0.7, edgecolors="k")
    b, a = np.polyfit(x, y, deg=1)
    xseq = np.linspace(xmin, xmax, num=100)
    ax.plot(xseq, a + b * xseq, color="k", lw=2.5);
    if xlabel: 
        plt.xlabel(xlabel)
    else: 
        plt.xlabel("True Parameter")
    if ylabel: 
        plt.ylabel(ylabel)
    else:
        plt.ylabel("Recovered Parameter")
    plt.suptitle(title)
    plt.title("Formula: y = {0} + {1}x".format(a, b))
    if save: 
        plt.savefig(save_path , dpi = 300)

def param_recovery_diagnostics(bounds, gen_func, llh_func, dim, title, params, 
                               nsamples = 100, msg = False, save = False, save_path = None):
    
    assert params.shape[0] == dim, "Mismatch in number of titles and parameters"
    results = parameter_recovery(bounds, gen_func, llh_func, msg = msg, nsamples = nsamples)
    
    for i in range(dim):
        true_param = results[:, 0, i]
        predict_param = results[:, 1, i]
        this_title = title + ": " + params[i]
        plot_param_recovery_analysis(true_param, predict_param, this_title,
                                     xmin = bounds[i][0], xmax = bounds[i][1], 
                                     save = save, save_path = save_path + "_" + params[i])
        
    return results
