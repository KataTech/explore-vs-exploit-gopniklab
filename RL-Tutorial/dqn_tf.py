import os 
import numpy as np 
import tensorflow as tf 

class DeepQNetwork(object): 
        """
        Purpose: This is the class for our Deep Q Network. It takes a batch of images 
                 from the environment (breakout, for the purposes of this tutorial), 
                 passes it through a convolutionary neural network (CNN) to perform 
                 the feature selection based on sequence of images, then passes to a 
                 fully connected neural network to determine the value of the given 
                 actions, and finally it utilizes the maximum value of the next action
                 to determine its loss function for training with backpropagation. 
        """

        def __init__(self, lr, n_actions, name, fcl_dims = 256, 
                        input_dims = (210, 160, 4), chkpt_dir = 'tmp/dpn'):
                """
                Purpose: Initialize the Deep Q Network. 
                Inputs: 
                        - lr: learning rate
                        - n_actions: number of actions 
                        - name: the name of the deep Q network, necessary variable
                                since separate networks are used for selecting an action
                                vs determining the value of an action
                        - fcl_dims: the number of dimensions in the first fully 
                                    connected layer
                        - input_dims: 3-tuple representing the input dimensions; 
                                      first two values correspond to image resolutions 
                                      and the last correspond to number of frames in 
                                      the stack at a time
                        - chkpt_dir: directory for saving the model
                """
                # saving the input parameters as object parameter
                self.lr = lr
                self.name = name
                self.n_actions = n_actions
                self.fcl_dims = fcl_dims
                self.input_dims = input_dims

                # instantiate tensorflow session and build the network
                self.sess = tf.Session()
                self.build_network()
                self.sess.run(tf.global_variables_initializer())
                self.saver = tf.train.Saver()
                self.check_point_file = os.path.join(chkpt_dir, 'deepqnet.ckpt')
                self.params = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, 
                                                scope=self.name)
        
        def build_network(self): 
                """
                Purpose: Build the neural network via tensorflow. 
                """
                
                with tf.variable_scope(self.name): 

                        # initialize variables for learning 
                        self.input = tf.placeholder(tf.float32, shape = [None, *self.input_dims], 
                                                        name = "inputs")
                        self.actions = tf.placeholder(tf.float32, shape = [None, self.n_actions], 
                                                        name = "action_taken")
                        self.q_target = tf.placeholder(tf.float32, shape = [None, self.n_actions])

                        # convolution neural network specifications 
                        conv1 = tf.layers.conv2d(inputs = self.input, filters = 32, kernel_size = (8, 8), 
                                                strides = 4, name = 'conv1', 
                                                kernel_initializer = tf.variance_scaling_initializer(scale = 2))
                        conv1_activated = tf.nn.relu(conv1)

                        conv2 = tf.layers.conv2d(inputs = conv1_activated, filters = 64, kernel_size = (4, 4), 
                                                strides = 2, name = 'conv2', 
                                                kernel_initializer = tf.variance_scaling_initializer(scale = 2))
                        conv2_activated = tf.nn.relu(conv2)

                        conv3 = tf.layers.conv2d(inputs = conv2_activated, filters = 128, kernel_size = (3, 3), 
                                                strides = 1, name = 'conv3', 
                                                kernel_initializer = tf.variance_scaling_initializer(scale = 2))
                        conv3_activated = tf.nn.relu(conv3)

                        # flatten CNN results and pass to dense network to get values of (state, action) pairs
                        flat = tf.layers.flatten(conv3_activated)
                        dense1 = tf.layers.dense(flat, units = self.fcl_dims, activation = tf.nn.relu,
                                                kernel_initializer = tf.variance_scaling_initializer(scale = 2))
                        self.Q_values = tf.layers.dense(dense1, units = self.n_actions, 
                                                        kernel_initializer = tf.variance_scaling_initializer(scale = 2))
                        self.q = tf.reduce_sum(tf.multiply(self.Q_values, self.actions))
                        
                        # specify the loss function and training operations 
                        self.loss = tf.reduce_mean(tf.square(self.q - self.q_target))
                        self.train_op = tf.train.AdamOptimizer(self.lr).minimize(self.loss)

        def load_checkpoint(self): 
                print('... loading checkpoint ...')
                self.saver.restore(self.sess, self.check_point_file)

        def save_checkpoint(self): 
                print('... saving checkpoint ...')
                self.saver.save(self.sess, self.check_point_file)


        class Agent(object): 

                def _init_(self, alpha, gamma, mem_size, n_actions, epsilon, batch_size,
                           replace_target = 5000, input_dims = (210, 160, 4), q_next = 'tmp/q_next', 
                           q_eval = 'tmp/q_eval'): 
                           
                        self.n_actions = n_actions   # number of actions 
                        self.action_space = [ i for i in range(self.n_actions) ] # set of actions 
                        self.gamma = gamma # how much to discount future rewards 
                        self.mem_size = mem_size # how many transitions to store our memory
                        self.epsilon = epsilon
                        self.batch_size = batch_size
                        self.replace_target = replace_target
                        self.mem_cntr = 0 # counter that keeps track of the number of memory stored
                        self.q_next = DeepQNetwork(alpha, n_actions, name='q_next', input_dims=input_dims, 
                                                   chkpt_dir=q_next)
                        self.q_eval = DeepQNetwork(alpha, n_actions, name='q_eval', input_dims=input_dims, 
                                                   chkpt_dir=q_eval)

                        # memory vector for Q-Learning
                        self.state_memory = np.zeros((self.mem_size, *input_dims))
                        self.new_state_memory = np.zeros((self.mem_size, *input_dims))
                        self.action_memory = np.zeros((self.mem_size, self.n_actions), dtype = np.int8)
                        self.reward_memory = np.zeros(self.mem_size)
                        self.terminal_memory = np.zeros(self.mem_size, dtype = np.int8)

                def store_transition(self, state, action, reward, state_, terminal): 

                        index = self.mem_cntr % self.mem_size
                        self.state_memory[index] = state
                        actions = np.zeros(self.n_actions)
                        actions[action] = 1.0 # one-hot encoding in progress
                        self.action_memory[index] = actions 
                        self.reward_memory[index] = reward
                        self.new_state_memory[index] = state_
                        self.terminal_memory[index] = terminal

                        self.mem_cntr += 1

                def choose_action(self, state):
                        """
                        Purpose: Implement an epsilon-greedy action selection procedure
                        """
                        rand = np.random.random()
                        if rand < self.epsilon: # select a random action
                                action = np.random.choice(self.action_space)
                        else:                   # determine the max valued action
                                actions = self.q_eval.sess.run(self.q_eval.Q_values, 
                                                               feed_dict = {self.q_eval.input: state})
                                action = np.argmax(actions)
                        return action

                def learn(self):

                        # verify if the target network need to be replaced 
                        if self.mem_cntr % self.replace_target == 0: 
                                self.update_graph()

                        # find out where the memory ends 
                        max_mem = self.mem_cntr if self.mem_cntr < self.mem_size else self.mem_size
                        batch = np.random.choice(max_mem, self.batch_size)
                        state_batch = self.state_memory[batch]
                        action_batch = self.action_memory[batch]
                        reward_batch = self.reward_memory[batch]
                        terminal_batch = self.terminal_memory[batch]

                        # convert one-hot encoding back to integer encoding via dot product
                        action_values = np.array([0, 1, 2], dtype = np.int8)
                        action_indices = np.dot(action_batch, action_values)                        

                        # calculate values of current and next set of states
                        q_eval = self.q_eval.sess.run(self.q_eval.Q_values, feed_dict={self.q_eval.input: state_batch})
                        q_next = self.q_next.sess.run(self.q_next.Q_values, feed_dict={self.q_next.input: new_state_batch})
                        q_target = q_eval.copy()
                        q_target[:, action_indices] = reward_batch + self.gamma * np.max(q_next, axis = 1) * terminal_batch
                        






                

