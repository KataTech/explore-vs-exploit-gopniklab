{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f36915ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# %matplotlib notebook \n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "from scipy.optimize import minimize\n",
    "from mpl_toolkits import mplot3d\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86025cf1",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f2915905",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the data sets from the approach/avoid task\n",
    "aa_adults = pd.read_csv(\"Study3_AAData_Adults.csv\", index_col = 0)\n",
    "aa_kids = pd.read_csv(\"Study3_AAData_Kids.csv\", index_col = 0)\n",
    "\n",
    "# subset by a preliminary set of interesting variables \n",
    "variables_of_interest = [\"PartID\", \"Block\", \"BadBlocks\", \"AgeGroup\", \"Trial\", \"TrialSet\", \"Valence\", \"AA\", \"Win\"]\n",
    "aa_adults = aa_adults[variables_of_interest]\n",
    "aa_kids = aa_kids[variables_of_interest]\n",
    "\n",
    "# modify the \"Win\" columns to capture the true Reward in terms of stickers/dollars\n",
    "aa_kids.loc[aa_kids.Win == 0, \"Win\"] = -2\n",
    "aa_kids.loc[aa_kids.AA == 0, \"Win\"] = 0\n",
    "aa_adults.loc[aa_adults.Win == 0, \"Win\"] = -2\n",
    "aa_adults.loc[aa_adults.AA == 0, \"Win\"] = 0\n",
    "\n",
    "# separate into kids_young, kids_old, adults \n",
    "aa_kids_young = aa_kids[aa_kids.AgeGroup == \"Young\"].copy()\n",
    "aa_kids_old = aa_kids[aa_kids.AgeGroup == \"Old\"].copy()\n",
    "\n",
    "# drop the AgeGroup column since they are meaningless \n",
    "aa_kids_young.drop(columns = [\"AgeGroup\"], inplace = True)\n",
    "aa_kids_old.drop(columns = [\"AgeGroup\"], inplace = True)\n",
    "aa_adults.drop(columns = [\"AgeGroup\"], inplace = True)\n",
    "\n",
    "# encode the kid participant IDs as numerics \n",
    "ptcp = 1\n",
    "ptcp_to_num_young = {}\n",
    "for participant in set(aa_kids_young.PartID): \n",
    "    ptcp_to_num_young[participant] = ptcp\n",
    "    ptcp += 1\n",
    "aa_kids_young.replace({\"PartID\": ptcp_to_num_young}, inplace = True)\n",
    "\n",
    "ptcp = 1\n",
    "ptcp_to_num_old = {}\n",
    "for participant in set(aa_kids_old.PartID): \n",
    "    ptcp_to_num_old[participant] = ptcp\n",
    "    ptcp += 1\n",
    "aa_kids_old.replace({\"PartID\": ptcp_to_num_old}, inplace = True)\n",
    "\n",
    "# sort each dataframe by participant ID and then Trial\n",
    "aa_kids_young.sort_values(by = [\"PartID\", \"Trial\"], inplace = True)\n",
    "aa_kids_old.sort_values(by = [\"PartID\", \"Trial\"], inplace = True)\n",
    "aa_adults.sort_values(by = [\"PartID\", \"Trial\"], inplace = True)\n",
    "\n",
    "# Check for NA Values\n",
    "variables_of_interest = [\"PartID\", \"Block\", \"BadBlocks\", \"Trial\", \"TrialSet\", \"Valence\", \"AA\", \"Win\"]\n",
    "aa_kids_young.loc[aa_kids_young[variables_of_interest].isna().any(axis=1), variables_of_interest] # Good!\n",
    "aa_kids_old.loc[aa_kids_old[variables_of_interest].isna().any(axis=1), variables_of_interest] # Good!\n",
    "aa_adults.loc[aa_adults[variables_of_interest].isna().any(axis=1), variables_of_interest] # abnormal entry spotted\n",
    "aa_adults.drop(index = 406, inplace = True) # drop the bad entry \n",
    "\n",
    "# reset the index after re-ordering\n",
    "aa_kids_young.reset_index(inplace = True, drop = True)\n",
    "aa_kids_old.reset_index(inplace = True, drop = True)\n",
    "aa_adults.reset_index(inplace = True, drop = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "554a7d34",
   "metadata": {},
   "source": [
    "## RL Model Generating Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e91699a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def RL_StateAction(param, numParticipants, trialSets): \n",
    "    \"\"\"\n",
    "    Generates simulated data with the two-factor Q-learning model, i.e. the Q function \n",
    "    takes as input both the block currently presented to the participant and the action \n",
    "    available to them overtime. This is an extension/modification of the RL function. \n",
    "    \"\"\"\n",
    "    # set-up\n",
    "    beta = param[0]\n",
    "    alpha = param[1]\n",
    "    blocks = {\"BP\", \"BS\", \"WP\", \"WS\"}\n",
    "    rewards = [-2.0, 1.0]\n",
    "    Data = pd.DataFrame([[1, 1, 1, 1, 1]], columns=[\"PartID\", \"Trial\", \"AA\", \"Reward\", \"Block\"])\n",
    "    \n",
    "    for participant in range(1, numParticipants + 1): \n",
    "        trial = 1 # counter for the trial number\n",
    "        bad_block = random.choice(tuple(blocks)) # select a bad block for the current participant\n",
    "        Q = pd.DataFrame(np.full((2, 4), 0.5), columns = list(blocks)) # set up Q-function\n",
    "        for trialset in range(1, trialSets + 1): \n",
    "            for block in blocks: \n",
    "                # determine approach vs. avoid - bernoulli trial\n",
    "                softmax = np.array([0.5, 0.5])\n",
    "                softmax[0] = 1 / np.sum(np.exp(beta * (Q[[block]].to_numpy() - Q.loc[0, block])))\n",
    "                softmax[1] = 1 - softmax[0]\n",
    "                action = np.random.choice(range(2), p=softmax)\n",
    "                # update values depending on action\n",
    "                if action == 0: # avoid\n",
    "                    reward = 0 \n",
    "                else: # approach\n",
    "                    reward = rewards[int(block != bad_block)]\n",
    "                    Q.loc[action, block] += alpha * (reward - Q.loc[action, block])\n",
    "                # store the data\n",
    "                temp_df = pd.DataFrame([[participant, trial, action, reward, block]], \n",
    "                                       columns=[\"PartID\", \"Trial\", \"AA\", \"Reward\", \"Block\"])\n",
    "                Data = pd.concat([Data, temp_df]) \n",
    "                trial += 1\n",
    "                \n",
    "    return Data[1:]  # remove first row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "33ffaec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def RL2a_StateAction(param, numParticipants, trialSets): \n",
    "    \"\"\"\n",
    "    Generates simulated data with the two-factor Q-learning model, i.e. the Q function \n",
    "    takes as input both the block currently presented to the participant and the action \n",
    "    available to them overtime. This is an extension/modification of the RL2a function. \n",
    "    \"\"\"\n",
    "    # set-up\n",
    "    beta = param[0]\n",
    "    alpha_pos = param[1]\n",
    "    alpha_neg = param[2]\n",
    "    blocks = {\"BP\", \"BS\", \"WP\", \"WS\"}\n",
    "    rewards = [-2.0, 1.0]\n",
    "    Data = pd.DataFrame([[1, 1, 1, 1, 1]], columns=[\"PartID\", \"Trial\", \"AA\", \"Reward\", \"Block\"])\n",
    "    \n",
    "    for participant in range(1, numParticipants + 1): \n",
    "        trial = 1 # counter for the trial number\n",
    "        bad_block = random.choice(tuple(blocks)) # select a bad block for the current participant\n",
    "        Q = pd.DataFrame(np.full((2, 4), 0.5), columns = list(blocks)) # set up Q-function\n",
    "        for trialset in range(1, trialSets + 1): \n",
    "            for block in blocks: \n",
    "                # determine approach vs. avoid - bernoulli trial\n",
    "                softmax = np.array([0.5, 0.5])\n",
    "                softmax[0] = 1 / np.sum(np.exp(beta * (Q[[block]].to_numpy() - Q.loc[0, block])))\n",
    "                softmax[1] = 1 - softmax[0]\n",
    "                action = np.random.choice(range(2), p=softmax)\n",
    "                # update values depending on action\n",
    "                if action == 0: # avoid\n",
    "                    reward = 0 \n",
    "                else: # approach\n",
    "                    reward = rewards[int(block != bad_block)]\n",
    "                    if reward > 0: \n",
    "                        Q.loc[action, block] += alpha_pos * (reward - Q.loc[action, block])\n",
    "                    else: \n",
    "                        Q.loc[action, block] += alpha_neg * (reward - Q.loc[action, block])    \n",
    "                # store the data\n",
    "                temp_df = pd.DataFrame([[participant, trial, action, reward, block]], \n",
    "                                       columns=[\"PartID\", \"Trial\", \"AA\", \"Reward\", \"Block\"])\n",
    "                Data = pd.concat([Data, temp_df]) \n",
    "                trial += 1\n",
    "                \n",
    "    return Data[1:]  # remove first row"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3faeff84",
   "metadata": {},
   "source": [
    "## RL Model Likelihood Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8690815d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def RL_LLH(param, Data): \n",
    "    \"\"\"\n",
    "    Effect: This function computes the negative loglikelihood of the softmax \n",
    "    equation of a two-alpha (positive and negative learning rate) RL model\n",
    "    given a set of parameters: beta, alpha. \n",
    "    \n",
    "    [llh]=RL_LLH(param) computes the negative log-likelihood of the data given\n",
    "    a simple softmax decision RL model with s singular leanring rate, and the parameters parm.\n",
    "    \n",
    "    Requires: \n",
    "    \n",
    "    Since the computation process requires an update with an iterator, it is \n",
    "    expected that the Data supplied contains ONLY the entries that are necessary\n",
    "    for the computation desired, i.e. if you are looking to compute LLH for the non-Zaffs\n",
    "    \n",
    "    param[0], beta, is the softmax temperature of the RL policy\n",
    "    param[1], alpha is the learning rate of RL agent for reward prediction errors.\n",
    "    \n",
    "    Data is a 2-D numpy array containing the data generated by the agent.\n",
    "    Data[:,0] is the participant ID (1 - n) at each trial\n",
    "    Data[:,1] is the the trial number in a block (1-N)\n",
    "    Data[:,2] is the agent's choice {0, 1} corresponding to avoid/approach at each trial\n",
    "    Data[:,3] is the feedback received by the agent at each trial {-2, 0, 1}\n",
    "    Data[:,4] is the identity of the object {0 - 3} presented for the trial\n",
    "    \n",
    "    nllh = -llh is the negative log likelihood.\n",
    "    \"\"\"\n",
    "    \n",
    "    # model parameters \n",
    "    beta = param[0]\n",
    "    alpha = param[1]\n",
    "    \n",
    "    # task parameters\n",
    "    n = int(np.max(Data[:, 0])) # number of participants\n",
    "    llh = 0\n",
    "    k = 0 \n",
    "    \n",
    "    # iterate over the participants (each should have different policies)\n",
    "    for participant in range(1, n + 1): \n",
    "        # initialize Q-values \n",
    "        Q = {\"BP\": -.5, \"BS\": -.5, \"WP\": -.5, \"WS\": -.5}\n",
    "        # iterate over all the trials of every participant\n",
    "        for trial in Data[Data[:, 0] == participant, 1]: \n",
    "            # set trial outcomes \n",
    "            action = Data[k, 2]\n",
    "            reward = Data[k, 3]\n",
    "            block = Data[k, 4]\n",
    "            # skip trial if the action is to avoid\n",
    "            if action == 0: \n",
    "                k += 1\n",
    "                continue\n",
    "            # compute softmax probabilities\n",
    "            vals = np.fromiter(Q.values(), dtype=float)\n",
    "            softmax = {\"BP\": 0, \"BS\": 0, \"WP\": 0, \"WS\": 0}\n",
    "            softmax[\"WS\"] = 1 / sum(np.exp(beta * (vals - Q['WS'])))\n",
    "            softmax[\"WP\"] = 1 / sum(np.exp(beta * (vals - Q['WP'])))\n",
    "            softmax[\"BS\"] = 1 / sum(np.exp(beta * (vals - Q['BS'])))\n",
    "            softmax[\"BP\"] = 1 - softmax[\"BS\"] - softmax[\"WP\"] - softmax[\"WS\"]\n",
    "            # update the policy\n",
    "            Q[block] += alpha * (reward - Q[block])\n",
    "            # update the log likelihood\n",
    "            llh += np.log(softmax[block])\n",
    "            k += 1\n",
    "            \n",
    "    return -llh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "263eb175",
   "metadata": {},
   "outputs": [],
   "source": [
    "def RL_StateAction_LLH(param, Data): \n",
    "    \"\"\"\n",
    "    This function is the loglikelihood function for a simple RL model with both state and action \n",
    "    build into the decision process. Specifically, the Q function takes as input both the block \n",
    "    currently presented to the participant and the action available to them. \n",
    "    \"\"\"\n",
    "    beta = param[0]\n",
    "    alpha = param[1] \n",
    "    n = int(np.max(Data[:, 0])) # number of participants \n",
    "    llh = 0 # log-likelihood \n",
    "    k = 0 # iteration tracker\n",
    "    blocks = {\"BP\": 0, \"BS\": 1, \"WP\": 2, \"WS\": 3}\n",
    "    \n",
    "    for participant in range(1, n + 1): \n",
    "        # set up Q-function\n",
    "        Q = np.full((2, 4), np.nan)\n",
    "        Q[0, :] = 0\n",
    "        Q[1, :] = -0.5\n",
    "        for trial in Data[Data[:, 0] == participant, 1]: \n",
    "            # set trial outcomes \n",
    "            action = Data[k, 2]\n",
    "            reward = Data[k, 3]\n",
    "            block = Data[k, 4]\n",
    "            block_idx = blocks[block]\n",
    "            # compute softmax probabilities\n",
    "            softmax = np.full(2, np.nan)\n",
    "            softmax[0] = 1 / np.sum(np.exp(beta * (Q[:, block_idx] - Q[0, block_idx])))\n",
    "            softmax[1] = 1 - softmax[0]\n",
    "            # update Q-value if the block was approached\n",
    "            if action == 1: \n",
    "                Q[action, block_idx] += alpha * (reward - Q[action, block_idx])\n",
    "            # update the loglikelihood \n",
    "            llh += np.log(softmax[int(action)])\n",
    "            # update iteration counter\n",
    "            k += 1\n",
    "            \n",
    "    return -llh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90af396f",
   "metadata": {},
   "source": [
    "## Optimize and Graphing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "dbd65e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize(fname, \n",
    "             bounds, \n",
    "             Data, \n",
    "             niter,\n",
    "             toplot=False, \n",
    "             message=False\n",
    "            ):\n",
    "            \n",
    "    \"\"\"\n",
    "    bestparameters,bestllh = optimize(fname, bounds, Data, niter, toplot) runs \n",
    "    the minimize function niter times on the function fname, with constraints \n",
    "    bounds to find parameters that best fit the data Data. It returns the \n",
    "    best likelihood and best fit parameters over the niter iterations.\n",
    "\n",
    "    ## fname is the python function to optimize. fname \n",
    "    should take as first argument a 1 by n vector of parameters. \n",
    "    Note the bounds are set up differently than they are in Matlab, \n",
    "    And should come as a list of [min,max] pairs. (ie. [[min,max],[min,max], ...])\n",
    "     \n",
    "    ## Data is the data set to be fit by likelihood function fname.\n",
    "    ## niter is the number of starting points for the optimization\n",
    "    ## toplot is an optional argument; if plot~=0, this function will plot the\n",
    "    best likelihood as a function of starting point iterations.\n",
    "    \n",
    "    ## best parameters is the 1*n vector of parameters found to minimize the\n",
    "    negative log likelihood over the data.\n",
    "    bestllh is the log likelihood value for the best parameters.\n",
    "     \n",
    "    \"\"\"\n",
    "    \n",
    "    outcomes = np.full([niter, len(bounds)+1], np.nan)\n",
    "    optimcurve = np.full(niter, np.nan)\n",
    "    for i in range(niter):\n",
    "        \n",
    "        if message: \n",
    "            if (int(niter * 0.10) == i): \n",
    "                print(\"Processed 10% ....\")\n",
    "            elif (int(niter * 0.20) == i): \n",
    "                print(\"Processed 20% ....\")\n",
    "            elif (int(niter * 0.30) == i): \n",
    "                print(\"Processed 30% ....\")\n",
    "            elif (int(niter * 0.40) == i): \n",
    "                print(\"Processed 40% ....\")\n",
    "            elif (int(niter * 0.50) == i): \n",
    "                print(\"Processed 50% ....\")\n",
    "            elif (int(niter * 0.60) == i): \n",
    "                print(\"Processed 60% ....\")\n",
    "            elif (int(niter * 0.70) == i): \n",
    "                print(\"Processed 70% ....\")\n",
    "            elif (int(niter * 0.80) == i): \n",
    "                print(\"Processed 80% ....\")\n",
    "            elif (int(niter * 0.90) == i): \n",
    "                print(\"Processed 90% ....\")\n",
    "        \n",
    "        # random starting point based on maximum bounds\n",
    "        params0 = np.array([bound[1] * np.random.rand() for bound in bounds])\n",
    "            \n",
    "        # compute the function value at the starting point\n",
    "        llh0 = fname(params0, Data)\n",
    "        \n",
    "        # run the optimizer with constraints\n",
    "        result = minimize(fun=fname, x0=params0, args=(Data), bounds=bounds)\n",
    "        x = result.x\n",
    "        bestllh = fname(x, Data)\n",
    "        outcomes[i, :] = [bestllh] + [xi for xi in x]    \n",
    "        optimcurve[i] = min(outcomes[:(i+1), 0])\n",
    "\n",
    "    if message: \n",
    "        print(\"Completed iterations... now compiling results...\")\n",
    "    \n",
    "    # find the global minimum out of all outcomes\n",
    "    i = np.argwhere(outcomes[:, 0] == np.min(outcomes[:, 0]))\n",
    "    bestparameters = outcomes[i[0], 1:].flatten()\n",
    "    bestllh = -1 * outcomes[i[0], 0].flatten()[0]\n",
    "    \n",
    "    # plot the best llh found by the optimizer as a function of iteration number.\n",
    "    if toplot:\n",
    "        plt.figure()\n",
    "        plt.plot(range(niter), np.round(optimcurve, 6), 'o-')\n",
    "        plt.yscale(\"log\")\n",
    "        plt.xlabel('iteration')\n",
    "        plt.ylabel('best minimum')\n",
    "        plt.title('Negative log likelihood')\n",
    "    \n",
    "    return(bestparameters, bestllh)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd36820f",
   "metadata": {},
   "source": [
    "## Testing!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b79ae6a7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         2177 function calls in 0.012 seconds\n",
      "\n",
      "   Ordered by: standard name\n",
      "\n",
      "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
      "        1    0.000    0.000    0.000    0.000 <__array_function__ internals>:2(amax)\n",
      "        1    0.009    0.009    0.012    0.012 <ipython-input-36-694b4defbb43>:1(RL_LLH)\n",
      "        1    0.000    0.000    0.012    0.012 <string>:1(<module>)\n",
      "        1    0.000    0.000    0.000    0.000 fromnumeric.py:2612(_amax_dispatcher)\n",
      "        1    0.000    0.000    0.000    0.000 fromnumeric.py:2617(amax)\n",
      "        1    0.000    0.000    0.000    0.000 fromnumeric.py:70(_wrapreduction)\n",
      "        1    0.000    0.000    0.000    0.000 fromnumeric.py:71(<dictcomp>)\n",
      "        1    0.000    0.000    0.012    0.012 {built-in method builtins.exec}\n",
      "     1299    0.002    0.000    0.002    0.000 {built-in method builtins.sum}\n",
      "        1    0.000    0.000    0.000    0.000 {built-in method numpy.core._multiarray_umath.implement_array_function}\n",
      "      433    0.001    0.000    0.001    0.000 {built-in method numpy.fromiter}\n",
      "        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}\n",
      "        1    0.000    0.000    0.000    0.000 {method 'items' of 'dict' objects}\n",
      "        1    0.000    0.000    0.000    0.000 {method 'reduce' of 'numpy.ufunc' objects}\n",
      "      433    0.000    0.000    0.000    0.000 {method 'values' of 'dict' objects}\n",
      "\n",
      "\n",
      "         8428 function calls in 0.015 seconds\n",
      "\n",
      "   Ordered by: standard name\n",
      "\n",
      "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
      "        1    0.000    0.000    0.000    0.000 <__array_function__ internals>:2(amax)\n",
      "      544    0.000    0.000    0.001    0.000 <__array_function__ internals>:2(copyto)\n",
      "      512    0.000    0.000    0.005    0.000 <__array_function__ internals>:2(sum)\n",
      "        1    0.007    0.007    0.015    0.015 <ipython-input-40-ef12554abc3a>:1(RL_StateAction_LLH)\n",
      "        1    0.000    0.000    0.015    0.015 <string>:1(<module>)\n",
      "      544    0.000    0.000    0.001    0.000 _asarray.py:23(asarray)\n",
      "      512    0.000    0.000    0.000    0.000 fromnumeric.py:2106(_sum_dispatcher)\n",
      "      512    0.001    0.000    0.004    0.000 fromnumeric.py:2111(sum)\n",
      "        1    0.000    0.000    0.000    0.000 fromnumeric.py:2612(_amax_dispatcher)\n",
      "        1    0.000    0.000    0.000    0.000 fromnumeric.py:2617(amax)\n",
      "      513    0.001    0.000    0.003    0.000 fromnumeric.py:70(_wrapreduction)\n",
      "      513    0.000    0.000    0.000    0.000 fromnumeric.py:71(<dictcomp>)\n",
      "      544    0.000    0.000    0.000    0.000 multiarray.py:1054(copyto)\n",
      "      544    0.001    0.000    0.003    0.000 numeric.py:288(full)\n",
      "        1    0.000    0.000    0.015    0.015 {built-in method builtins.exec}\n",
      "      512    0.000    0.000    0.000    0.000 {built-in method builtins.isinstance}\n",
      "      544    0.001    0.000    0.001    0.000 {built-in method numpy.array}\n",
      "     1057    0.001    0.000    0.005    0.000 {built-in method numpy.core._multiarray_umath.implement_array_function}\n",
      "      544    0.001    0.000    0.001    0.000 {built-in method numpy.empty}\n",
      "        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}\n",
      "      513    0.000    0.000    0.000    0.000 {method 'items' of 'dict' objects}\n",
      "      513    0.002    0.000    0.002    0.000 {method 'reduce' of 'numpy.ufunc' objects}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import a performance testing library\n",
    "import cProfile\n",
    "\n",
    "Data = aa_kids_young[[\"PartID\", \"Trial\", \"AA\", \"Win\", \"Block\"]].to_numpy()\n",
    "param = [0.5, 0.5]\n",
    "cProfile.run(\"RL_LLH(param, Data)\")\n",
    "cProfile.run(\"RL_StateAction_LLH(param, Data)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
